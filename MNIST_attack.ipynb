{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['http_proxy']=\"http://172.17.0.1:7890\"\n",
    "os.environ['https_proxy']=\"http://172.17.0.1:7890\"\n",
    "os.environ['all_proxy']=\"http://172.17.0.1:7890\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import foolbox as fb\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "from pytorchcv.model_provider import get_model as ptcv_get_model\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision.transforms import transforms\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST datasets in pytorch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_train = MNIST('./data',download=True,transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ]))\n",
    "\n",
    "data_test = MNIST('./data',train = False,download=True,transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ]))\n",
    "\n",
    "trainLoader = DataLoader(data_train, batch_size=64,shuffle=True)\n",
    "testLoader = DataLoader(data_test, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.6450,\n",
      "           1.9305,  1.5996,  1.4978,  0.3395,  0.0340, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  2.4015,\n",
      "           2.8088,  2.8088,  2.8088,  2.8088,  2.6433,  2.0960,  2.0960,\n",
      "           2.0960,  2.0960,  2.0960,  2.0960,  2.0960,  2.0960,  1.7396,\n",
      "           0.2377, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.4286,\n",
      "           1.0268,  0.4922,  1.0268,  1.6505,  2.4651,  2.8088,  2.4396,\n",
      "           2.8088,  2.8088,  2.8088,  2.7578,  2.4906,  2.8088,  2.8088,\n",
      "           1.3577, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.2078,  0.4159, -0.2460,\n",
      "           0.4286,  0.4286,  0.4286,  0.3268, -0.1569,  2.5797,  2.8088,\n",
      "           0.9250, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242,  0.6322,  2.7960,  2.2360,\n",
      "          -0.1951, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.1442,  2.5415,  2.8215,  0.6322,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242,  1.2177,  2.8088,  2.6051,  0.1358,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242,  0.3268,  2.7451,  2.8088,  0.3649, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242,  1.2686,  2.8088,  1.9560, -0.3606, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.3097,  2.1851,  2.7324,  0.3140, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242,  1.1795,  2.8088,  1.8923, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           0.5304,  2.7706,  2.6306,  0.3013, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.1824,\n",
      "           2.3887,  2.8088,  1.6887, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.3860,  2.1596,\n",
      "           2.8088,  2.3633,  0.0213, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.0595,  2.8088,\n",
      "           2.8088,  0.5559, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.0296,  2.4269,  2.8088,\n",
      "           1.0395, -0.4115, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242,  1.2686,  2.8088,  2.8088,\n",
      "           0.2377, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242,  0.3522,  2.6560,  2.8088,  2.8088,\n",
      "           0.2377, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242,  1.1159,  2.8088,  2.8088,  2.3633,\n",
      "           0.0849, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242,  1.1159,  2.8088,  2.2105, -0.1951,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242]]])\n",
      "tensor([[[-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.2842,  0.0085,  1.0777, -0.1187,\n",
      "          -0.3606, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           2.8215,  2.0705, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242,  1.9814,  2.7960,  2.6815,  1.7905,\n",
      "          -0.2078, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.7850,\n",
      "           2.7451,  0.5304, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242,  2.5033,  2.7960,  1.7905, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.4413,  2.7069,\n",
      "           2.1087, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242,  2.5033,  2.7960,  0.7468, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.0933,  2.1596,  2.7833,\n",
      "          -0.1569, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242,  2.5033,  2.6687,  0.2122, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242,  1.7905,  2.7833,  2.1214,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242,  2.5160,  2.5160, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242,  0.9759,  2.7960,  2.1596, -0.2842,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242,  2.5033,  2.5033, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242,  0.0340,  2.3633,  2.7833,  1.3450, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242,  2.5033,  2.5033, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242,  1.7396,  2.7833,  2.1469,  0.4668,  0.0595,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242,  2.5033,  2.7451,  0.5304, -0.4242,\n",
      "          -0.4242,  0.7850,  2.7960,  2.7833,  2.6306,  2.2487,  0.0213,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242,  1.8414,  2.7960,  2.3378, -0.0169,\n",
      "           0.1231,  2.5033,  2.7960,  2.7833,  2.3887,  0.6959, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242,  0.2504,  2.6815,  2.7960,  2.7960,\n",
      "           2.7960,  2.7960,  2.7833,  2.2360, -0.0169, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242,  0.2122,  1.7778,  2.7833,\n",
      "           2.7833,  2.5160,  0.6450, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.5686,  2.7833,\n",
      "           2.4651,  0.8486, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242,  0.3649,  2.5797,  2.7833,\n",
      "           0.4159, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242,  2.0069,  2.7833,  2.3887,\n",
      "          -0.1060, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242,  0.3904,  2.8215,  2.7451,  1.0395,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.2078,  2.0323,  2.7960,  1.9305, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242,  1.8160,  2.7833,  2.7960,  2.2360,  1.6759,\n",
      "          -0.1569, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.0933,  2.4269,  2.7833,  2.7960,  2.5669,  0.3904,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.3351,  2.2614,  2.7833,  1.8669, -0.1951, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242]]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242,  1.3705, -0.0933,  2.0069,\n",
      "           0.2122, -0.4242, -0.4242, -0.0933,  0.0595,  0.3777,  1.0268,\n",
      "           2.0069,  1.0141,  0.8613,  0.3777,  0.3777, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  0.8613,  2.6306,  2.5160,  2.7833,\n",
      "           2.5924,  2.4524,  0.8486,  2.1214,  2.5542,  2.6306,  2.7960,\n",
      "           2.7833,  2.7833,  2.7451,  2.6306,  1.3577,  0.5431, -0.1824,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  2.4015,  2.7833,  2.7833,  2.2996,\n",
      "           2.4651,  2.7960,  2.7833,  2.7833,  2.7833,  2.7833,  2.7960,\n",
      "           2.7833,  2.7833,  2.7833,  2.7833,  2.1596,  2.3887,  0.1231,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  1.4341,  2.7451,  2.3760,  1.4850,\n",
      "           2.3124,  2.7960,  2.7833,  2.7833,  2.7833,  2.7833,  2.7960,\n",
      "           2.7833,  2.7833,  2.7833,  2.7833,  1.3577,  2.0832,  0.2504,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242,  2.4396,  2.7833,  2.7833,\n",
      "           2.7833,  2.7960,  2.7833,  2.7833,  2.7833,  2.7833,  2.7960,\n",
      "           2.7833,  2.7833,  2.7833,  2.7833,  2.1596,  2.6306,  2.0578,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242,  1.1795,  2.4906,  2.7960,\n",
      "           2.7960,  2.8215,  2.7960,  2.7960,  2.7960,  2.7960,  2.8215,\n",
      "           1.9942,  2.7960,  2.7960,  2.7960,  2.8215,  2.7960,  2.0705,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  2.0832,  2.6560,\n",
      "           0.7086, -0.0806,  1.5105,  1.3068,  1.1286,  0.7086, -0.0806,\n",
      "          -0.2715,  0.5686,  2.7833,  2.7833,  2.7960,  2.7833,  2.4651,\n",
      "           0.1740, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.1231,  0.1995,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242,  0.3013,  2.7833,  2.7833,  2.7960,  2.7833,  2.0578,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242,  0.3013,  2.7833,  2.7833,  2.7960,  2.7833,  2.0578,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242,  0.3013,  2.7833,  2.7833,  2.7960,  2.7833,  1.0777,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242,  2.0960,  2.7960,  2.7960,  2.8215,  2.7960,  0.2886,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242,  2.0832,  2.7833,  2.7833,  2.7960,  2.7833,  0.2758,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242,  2.0832,  2.7833,  2.7833,  2.7960,  2.7833,  0.2758,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242,  2.0832,  2.7833,  2.7833,  2.7960,  2.7833,  0.2758,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242,  1.0904,  2.7833,  2.7833,  2.7960,  2.7833,  0.2758,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.0169,  1.5232,  2.7960,  2.8215,  2.7960,  2.0705,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.3860,  1.5105,  2.7960,  2.7833,  2.2996,\n",
      "          -0.0678, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242,  1.3577,  2.7960,  2.7833,  2.7833,\n",
      "           2.3124,  1.6759, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242,  0.1740,  1.6378,  2.5542,  2.7833,\n",
      "           2.5797,  1.0013, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  1.4341,  2.3760,\n",
      "           0.2122,  0.2122, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242]]])\n",
      "tensor([[[-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.2969,  1.1541,  2.2233,  2.8215,\n",
      "           2.8088,  2.6433,  1.5614,  0.1231, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242,  0.1486,  2.5415,  2.8088,  2.8088,  2.8088,\n",
      "           2.8088,  2.8088,  2.8088,  2.4015, -0.2715, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242,  1.8541,  2.8088,  2.3887,  1.7014,  0.5304,\n",
      "           0.5304,  1.9432,  2.8088,  2.3505, -0.2969, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242,  0.4413,  2.8088,  2.3124, -0.0551, -0.4242,\n",
      "          -0.4242, -0.2715,  0.4795,  0.3013, -0.4242, -0.4242, -0.4242,\n",
      "          -0.3606, -0.2587, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.2969,  2.3760,  2.8088,  2.1596, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.0849,\n",
      "           1.7523,  1.9051, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242,  0.0849,  2.6178,  2.7197,  1.3450,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.3478,  1.0904,  2.7324,\n",
      "           2.7197,  0.9886, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242,  0.1104,  2.7197,  2.7069,\n",
      "           1.3450, -0.4242, -0.3478,  0.7213,  2.2233,  2.8088,  2.2105,\n",
      "           0.2122, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.4031,  2.8088,\n",
      "           2.7069,  0.9123,  1.7269,  2.8088,  2.8088,  1.5868, -0.3478,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.3478,  1.2432,\n",
      "           2.8088,  2.8088,  2.8088,  2.7069,  1.1795, -0.3478, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.3606,  0.8232,\n",
      "           2.8088,  2.8088,  2.7069,  0.6959, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.3606,  1.1668,  2.8088,\n",
      "           2.8088,  2.8088,  2.2233, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242,  0.6704,  2.8088,  2.8088,\n",
      "           2.3887,  2.8088,  2.7197, -0.0169, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.2715,  2.4396,  2.8088,  2.2105,\n",
      "           0.1358,  2.8088,  2.8088,  0.2758, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242,  0.6831,  2.8088,  2.4778, -0.2206,\n",
      "           0.0595,  2.8088,  2.8088,  0.5177, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.2969,  2.4524,  2.8088,  1.8160, -0.4242,\n",
      "           0.0595,  2.8088,  2.6560, -0.0678, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242,  0.7086,  2.8088,  2.6178, -0.0296, -0.3097,\n",
      "           1.8287,  2.8088,  2.2233, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.3224,  2.4524,  2.8088,  2.1851, -0.4242,  0.9504,\n",
      "           2.8088,  2.6560,  0.4413, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.3606,  2.2487,  2.8088,  1.7396,  0.5686,  2.7706,\n",
      "           2.8088,  1.2941, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242,  0.9123,  2.8088,  2.7960,  2.7833,  2.6687,\n",
      "           1.4723, -0.3988, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.2715,  2.1723,  2.8088,  1.8541,  0.1995,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242]]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           0.3013,  1.3705,  2.8215,  2.0960,  1.7396,  0.3013, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  1.0268,\n",
      "           1.3705,  0.3013, -0.4242, -0.4242,  0.6704,  2.4524,  1.3705,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.0551,  1.7396,  1.0268,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.0551,  1.7396,\n",
      "           1.7396, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242,  2.0960,  1.3705, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.0551,\n",
      "           2.4524, -0.0551, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242,  0.3013,  2.4524, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.6704,\n",
      "           2.8215, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242,  2.8215,  0.3013, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.3013,  2.8215,\n",
      "           2.8215,  2.8215,  1.7396, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242,  0.3013,  2.8215, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242,  0.3013,  2.8215,  2.8215,\n",
      "           2.8215,  1.7396, -0.0551, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242,  0.6704,  2.8215, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242,  0.3013,  2.0960,  2.8215,  2.8215,  2.0960,\n",
      "           0.3013, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242,  0.3013,  2.8215,  0.3013, -0.4242, -0.4242,\n",
      "          -0.0551,  2.0960,  2.8215,  2.4524,  1.0268, -0.0551, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242,  1.7396,  2.8215, -0.0551,  1.0268,\n",
      "           2.8215,  2.8215,  0.6704, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242,  2.8215,  2.8215,  2.8215,\n",
      "           1.0268, -0.0551, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242,  1.0268,  2.8215,  2.0960,  2.8215,\n",
      "          -0.0551, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242,  2.0960,  1.7396,  0.3013, -0.4242,  0.3013,\n",
      "           2.4524,  1.3705, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.0551,  0.6704,  1.7396, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.0551,  2.8215, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           2.4524,  0.6704, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242,  2.8215, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  1.0268,\n",
      "           2.4524, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.0551,  2.8215, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  1.0268,\n",
      "           0.6704, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.0551,\n",
      "           2.4524,  2.4524, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  2.8215,\n",
      "           0.3013, -0.4242, -0.4242, -0.4242, -0.4242,  0.3013,  2.4524,\n",
      "           2.4524, -0.0551, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.6704,\n",
      "           2.4524,  0.6704,  0.6704,  1.7396,  2.8215,  2.8215,  1.0268,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           1.3705,  1.7396,  2.8215,  1.7396,  0.6704, -0.0551, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242]]])\n",
      "tensor([[[-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.3478,  1.6632,  2.8088,  2.8088,\n",
      "          -0.2715, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.3733,  1.0650,  2.5924,  2.7960,  2.7960,  2.7324,\n",
      "          -0.2842, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.3478,\n",
      "          -0.1824,  1.5359,  2.7960,  2.7960,  2.7960,  2.6942,  0.4795,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.3478, -0.1060,  1.4468,\n",
      "           2.7960,  2.7960,  2.7960,  2.7960,  2.7960,  1.5868, -0.3988,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.0296,  1.8287,  2.7960,  2.7960,\n",
      "           2.7960,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960, -0.1951,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242,  1.2177,  2.7960,  2.7960,  2.7960,\n",
      "           2.6560,  2.3251,  2.3251,  2.7324,  2.7960,  2.7960,  0.8104,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242,  1.7905,  2.7960,  2.6687,  1.8160,\n",
      "           0.3522, -0.4242, -0.4242,  2.0705,  2.7960,  2.7960,  1.1159,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242,  0.5940,  2.1596,  0.7722, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242,  1.0777,  2.7960,  2.7960, -0.1951,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.1696,  2.4396,  2.7960,  2.7960, -0.1951,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242,  1.0777,  2.7960,  2.7960,  1.8160, -0.3606,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242,  0.7850,  2.6687,  2.7960,  2.7960,  1.0395, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.1442,  2.3633,  2.7960,  2.7960,  1.3450, -0.3478, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.2206,  1.7269,  2.1851,  2.1851,\n",
      "           2.3633,  2.7960,  2.7960,  2.7960, -0.0424, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.2842,  0.6959,  0.9632,  2.3633,  2.7960,  2.7960,  2.7960,\n",
      "           2.7960,  2.7960,  2.7960,  2.7960, -0.0424, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.2758,\n",
      "           1.7778,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n",
      "           2.7960,  2.7960,  2.7960,  2.7960,  2.3887,  0.4286, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.4159,  2.6433,\n",
      "           2.7960,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n",
      "           2.7960,  2.7960,  2.7960,  2.7960,  2.7960,  1.4596, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.3478,  1.9814,  2.7960,\n",
      "           2.7960,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,  2.4396,\n",
      "           1.0013, -0.1187,  0.4286,  2.6051,  2.7960,  1.4596, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.2587,  2.7960,  2.7960,\n",
      "           2.7960,  2.7960,  2.7960,  2.7960,  2.7960,  1.7523,  0.1231,\n",
      "          -0.4242, -0.4242, -0.4242, -0.2333,  0.5813, -0.2842, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.2587,  2.7960,  2.7960,\n",
      "           2.7960,  2.7960,  2.7960,  2.7960,  1.3832, -0.3860, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.3351,  1.3323,  2.7960,\n",
      "           2.7960,  2.7960,  1.7523,  0.1486, -0.3988, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242]]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.2842,  2.8215,  1.5996,\n",
      "          -0.3351, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242,  1.9814,  2.7960,  2.7833,\n",
      "           0.4540, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.0933,  2.5797,  2.7960,  2.7833,\n",
      "           0.4540, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242,  0.7595,  2.7833,  2.7960,  2.2996,\n",
      "          -0.2587, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242,  2.0832,  2.7833,  2.7960,  1.5232,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.2842,  2.1469,  2.5287,  0.1867, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.3351,  2.2742,  2.7960,  2.8215,  0.7468,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242,  1.9814,  2.7960,  2.7833,  1.9051, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242,  0.4668,  2.7833,  2.7833,  2.7960,  0.7468,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           0.8486,  2.7069,  2.7960,  2.6306,  0.6068, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242,  1.5232,  2.7833,  2.7833,  2.7197,  0.4286,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.3013,\n",
      "           2.6815,  2.7833,  2.7960,  2.1978, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242,  1.9178,  2.7833,  2.7833,  2.5033, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  1.7905,\n",
      "           2.7833,  2.7833,  2.7960,  0.8741, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242,  1.9178,  2.7833,  2.7833,  1.1795, -0.4242,\n",
      "          -0.0169,  1.0523, -0.2842, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.3013,  2.7960,\n",
      "           2.7960,  2.7960,  1.7141, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.3860,  1.0777,  2.7960,  2.7960,  2.7960,  1.2050,  0.8104,\n",
      "           2.4015,  2.7960, -0.1315, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.3606,  1.8414,  2.7833,\n",
      "           2.7833,  2.7833,  2.1596,  1.6250,  1.6250,  1.6250,  1.6250,\n",
      "           1.7396,  2.7833,  2.7833,  2.7833,  2.7833,  2.7960,  2.7833,\n",
      "           2.7833,  2.7833, -0.1442, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242,  0.9504,  2.7833,  2.7833,\n",
      "           2.7833,  2.7833,  2.7960,  2.7833,  2.7833,  2.7833,  2.7833,\n",
      "           2.7960,  2.7833,  2.7833,  2.7833,  2.7833,  2.7960,  2.7833,\n",
      "           2.5415,  1.3705, -0.3478, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242,  1.3323,  2.7833,  2.7833,\n",
      "           2.7833,  2.7833,  2.7960,  2.7833,  2.7833,  2.7833,  2.7833,\n",
      "           2.7960,  2.7833,  2.7833,  2.7833,  2.7833,  1.6250,  1.6123,\n",
      "           0.3904, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242,  0.6704,  2.7833,  2.7833,\n",
      "           2.7833,  2.7833,  2.7960,  2.7833,  2.7833,  1.5741,  1.3196,\n",
      "           2.1342,  2.7833,  2.7833,  2.7833,  0.7850, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.1442,  1.0523,\n",
      "           1.0523, -0.2842, -0.4242, -0.4242, -0.4242, -0.4242, -0.2842,\n",
      "           2.1469,  2.7960,  2.7960,  2.7960, -0.1315, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.7850,\n",
      "           2.7960,  2.7833,  2.7833,  2.0323, -0.3224, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.6577,\n",
      "           2.7960,  2.7833,  2.7833,  1.6123, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  1.0523,\n",
      "           2.7960,  2.7833,  2.7833,  1.6123, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.2842,\n",
      "           1.4596,  2.7833,  1.7141,  0.0213, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242]]])\n",
      "tensor([[[-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.1060,  2.1469,  2.8088,  2.8088,  2.5924,\n",
      "           0.2122, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242,  0.6831,  2.3760,  2.7960,  2.7960,  2.7960,  2.7960,\n",
      "           2.5033,  0.1740, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.3860,\n",
      "          -0.2333,  2.1469,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n",
      "           2.7960,  1.6505, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  1.0650,\n",
      "           2.7960,  2.8088,  2.7960,  2.7960,  2.7960,  1.4341,  2.3505,\n",
      "           2.7960,  1.6505, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  1.1668,  2.7069,\n",
      "           2.7960,  2.8088,  2.7960,  2.0578, -0.0551, -0.3733,  1.8541,\n",
      "           2.7960,  1.6505, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.3733,  0.9759,  2.7451,  2.7960,\n",
      "           2.7960,  2.8088,  2.7069,  0.5813, -0.4242, -0.4242,  1.8541,\n",
      "           2.7960,  1.6505, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242,  0.9759,  2.7960,  2.7960,  2.7960,\n",
      "           2.7960,  2.8088,  1.0268, -0.4242, -0.4242, -0.4242,  1.8541,\n",
      "           2.7960,  1.6505, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.0806,  2.1978,  2.7960,  2.7960,  2.7960,\n",
      "           2.7960,  0.8868, -0.3860, -0.4242, -0.4242, -0.0806,  2.1978,\n",
      "           2.7960,  1.6505, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.1442,  0.9377,  2.7960,  2.7960,  2.7960,  2.6942,\n",
      "           1.1032, -0.4242, -0.4242, -0.4242, -0.4242,  1.9942,  2.7960,\n",
      "           2.7960,  1.6505, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242,  1.1032,  2.7960,  2.7960,  2.7960,  2.7960,  1.1032,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242,  0.4922,  2.6433,  2.7960,\n",
      "           2.7960,  1.6505, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.2206,  2.8088,  2.8088,  2.8088,  2.8088,  2.3378,  0.0213,\n",
      "          -0.4242, -0.4242, -0.4242, -0.3351,  1.8160,  2.8088,  2.8088,\n",
      "           2.8088, -0.2460, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           0.2249,  2.7960,  2.7960,  2.7960,  2.3887,  0.0467, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242,  1.3450,  2.7960,  2.7960,  2.7960,\n",
      "           2.3887, -0.2842, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.3733,\n",
      "           1.7396,  2.7960,  2.7960,  2.7706,  1.0013, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.3733,  1.7396,  2.7960,  2.7960,  2.5415,\n",
      "           0.1104, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.3395,\n",
      "           2.7960,  2.7960,  2.7960,  2.4142, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242,  1.2432,  2.7960,  2.7960,  2.7960,  2.4142,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.3395,\n",
      "           2.7960,  2.7960,  2.7960,  1.8541, -0.4242, -0.4242, -0.2460,\n",
      "           0.5304,  2.4524,  2.7324,  2.7960,  2.7960,  2.5160,  0.4540,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.3395,\n",
      "           2.7960,  2.7960,  2.7960,  0.7850, -0.2333, -0.0551,  1.5868,\n",
      "           2.7960,  2.8088,  2.7960,  2.7960,  2.7960,  0.4922, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.3395,\n",
      "           2.7960,  2.7960,  2.7960,  2.7960,  1.9942,  2.7197,  2.7960,\n",
      "           2.7960,  2.8088,  2.7960,  2.7960,  1.9814, -0.2460, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.3522,\n",
      "           2.7960,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n",
      "           2.7960,  2.8215,  2.7960,  1.9432, -0.3224, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.3395,\n",
      "           2.7960,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,  2.6942,\n",
      "           2.2233,  1.4468,  0.3268, -0.1569, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.1060,\n",
      "           1.0141,  2.7960,  2.7960,  2.7960,  2.7960,  2.1214,  0.6704,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242]]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           0.0595,  0.7850,  1.0904,  0.9504,  0.3013,  0.3013, -0.0169,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.0595,  0.8232,\n",
      "           2.5542,  2.7833,  2.7833,  2.7960,  2.7833,  2.7833,  2.4778,\n",
      "           1.9305,  0.6577, -0.1824, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242,  0.0595,  1.8032,  2.7960,\n",
      "           2.7833,  2.7833,  2.1469,  1.5232,  2.7833,  2.7833,  2.7833,\n",
      "           2.7960,  2.7833,  2.5160,  1.8032, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.1824,  2.1723,  2.7960,  2.7706,\n",
      "           1.8032, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.2249,\n",
      "           0.8613,  0.5431,  1.0141, -0.2587, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.0169,  1.9560,  2.7833,  2.7833,  1.8032,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242,  0.2249,  2.4906,  2.7833,  2.7833,  1.6632, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242,  2.1342,  2.7960,  2.7833,  1.1923, -0.2333, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           0.1867,  2.6433,  2.8215,  2.4778,  0.1358, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           0.4159,  2.3887,  2.7960,  2.7833,  2.4269,  1.0904,  0.3013,\n",
      "           0.1358, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.1060,  1.8414,  2.7833,  2.7833,  2.7833,  2.7960,\n",
      "           2.6306,  2.0832,  2.0832,  0.6577, -0.0678, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.3733,  0.8868,  1.3450,  1.3450,  2.7960,\n",
      "           2.7833,  2.7833,  2.7833,  2.7960,  2.5542,  1.9560, -0.2587,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242,  0.0595,  1.0141,  1.1795,  2.6815,  2.7960,  0.7341,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.2587,\n",
      "           0.3013, -0.0169, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242,  0.1486,  2.4396,  2.7833,  1.0395,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.1867,  1.7650,\n",
      "           2.4269,  0.7722, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242,  0.4286,  1.2814,  2.4906,  2.7833,  1.9432, -0.1824,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  2.0069,  2.7833,\n",
      "           2.6687,  1.1668,  1.0268,  0.0595,  0.7086,  1.0141,  1.1795,\n",
      "           2.4396,  2.7069,  2.7833,  2.7960,  1.8287, -0.0296, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.5813,  2.7960,\n",
      "           2.7960,  2.7960,  2.8215,  2.7960,  2.7960,  2.7960,  2.8088,\n",
      "           2.7960,  2.6815,  2.4396,  1.0268,  0.0595, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.2969,  0.6450,\n",
      "           0.6450,  1.2686,  2.1596,  2.7833,  2.7833,  2.1469,  2.0832,\n",
      "           1.1159,  0.2886, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.3351,  0.2886,  0.2886, -0.3478, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242]]])\n",
      "tensor([[[-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242,  0.6704,  1.5487,  2.6178,  2.8215,  0.7341, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.4031,  0.9886,\n",
      "           2.2233,  2.6942,  2.8088,  2.7960,  2.7960,  2.3887, -0.0424,\n",
      "          -0.2969, -0.3860, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.1696,  1.6632,  2.7451,  2.8088,\n",
      "           2.7960,  2.4396,  0.7086,  0.7086,  1.5487,  2.4651,  0.8104,\n",
      "           2.7197,  1.4978, -0.3860, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242,  0.1358,  1.8032,  2.8088,  2.8088,  2.6051,\n",
      "           1.0523, -0.1696, -0.4242, -0.4242, -0.3478,  1.9560,  2.8088,\n",
      "           2.8088,  2.7069,  0.1867, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242,  0.1358,  2.4906,  2.7960,  2.8088,  1.7014, -0.1824,\n",
      "          -0.4242, -0.4242, -0.4242, -0.3606,  1.6505,  2.8088,  2.7960,\n",
      "           2.6687,  0.5559, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242,  0.7595,  2.7960,  2.7960,  2.0960, -0.3351, -0.4242,\n",
      "          -0.4242, -0.2587,  0.5686,  2.5160,  2.7960,  2.8088,  2.5287,\n",
      "           1.0395, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.1442,  2.5160,  2.7960,  2.7960,  1.7396,  0.6704,  0.6704,\n",
      "           1.4087,  2.2869,  2.8088,  2.7960,  2.7960,  2.6942,  0.2122,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.0424,  2.5669,  2.7960,  2.7960,  2.7960,  2.7960,  2.8088,\n",
      "           2.7960,  2.7960,  2.8088,  2.7960,  2.7960,  1.0904, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242,  1.8923,  2.8088,  2.8088,  2.8088,  2.8088,  2.3505,\n",
      "           2.3124,  2.8088,  2.8088,  2.7324,  1.2432, -0.3351, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.3988, -0.3733, -0.3733,  0.5559, -0.2969,  0.0976,\n",
      "           1.7396,  2.7960,  2.6560,  2.0069, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.1824,  2.3633,\n",
      "           2.7960,  2.7960,  1.9942, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  2.1469,  2.8088,\n",
      "           2.8088,  1.9432, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.3988,  1.1286,  2.7706,  2.8088,\n",
      "           2.7706,  0.8613, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242,  0.3140,  2.7960,  2.7960,  2.7960,\n",
      "           1.3196, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242,  0.4286,  2.6178,  2.8088,  2.8088,  0.6195,\n",
      "          -0.2715, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242,  0.1613,  2.1342,  2.7960,  2.7960,  2.0451, -0.1951,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           0.1995,  2.5033,  2.8088,  2.8088,  1.1414, -0.3097, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.3733,\n",
      "           2.0705,  2.8088,  2.8088,  2.5160, -0.1951, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.4413,\n",
      "           2.8088,  2.8088,  2.4015,  0.4286, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.1187,\n",
      "           1.9051,  2.7960,  1.5741, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242]]])\n"
     ]
    }
   ],
   "source": [
    "for imgs,labs in testLoader:\n",
    "    print(imgs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "log_interval = 10\n",
    "test_counter = [i*len(trainLoader.dataset) for i in range(10 + 1)]\n",
    "\n",
    "model = Net()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01,momentum=0.5)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(trainLoader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(trainLoader.dataset),\n",
    "            100. * batch_idx / len(trainLoader), loss.item()))\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append((batch_idx*64) + ((epoch-1)*len(trainLoader.dataset)))\n",
    "#         torch.save(model.state_dict(), '/results/model.pth')\n",
    "#         torch.save(optimizer.state_dict(), '/results/optimizer.pth')\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in testLoader:\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "            test_loss /= len(testLoader.dataset)\n",
    "            test_losses.append(test_loss)\n",
    "            print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "                test_loss, correct, len(testLoader.dataset),\n",
    "                100. * correct / len(testLoader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-3a1a5a310de2>:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.2356, Accuracy: 108/10000 (1%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2358, Accuracy: 214/10000 (2%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2356, Accuracy: 320/10000 (3%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2360, Accuracy: 415/10000 (4%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2366, Accuracy: 510/10000 (5%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2357, Accuracy: 612/10000 (6%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2355, Accuracy: 719/10000 (7%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2359, Accuracy: 822/10000 (8%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2352, Accuracy: 929/10000 (9%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1804, Accuracy: 1010/10000 (10%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.287640\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.289778\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.264085\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.280704\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.247383\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.210376\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.139359\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.083879\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.080602\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 1.883032\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.793057\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 1.583377\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 1.552345\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 1.415361\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 1.522289\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 1.267314\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.543485\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 1.213679\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 1.128868\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 1.242740\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.239435\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 1.172083\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.967863\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 1.040381\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.848667\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 1.069773\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.825999\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 1.079021\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.808915\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 1.142918\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.066618\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.799594\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.914819\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.637284\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.662449\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.970181\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.833846\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.630190\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.962364\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.567153\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.835405\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.635032\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.705698\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.616215\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.721400\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 1.017011\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.591121\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.588609\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.817770\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.778106\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.752632\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.624560\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.627868\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.512496\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.617472\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.651482\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.508312\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.653740\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.491383\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.513996\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.638495\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.615699\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.701808\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.639606\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.542107\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.638630\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.620243\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.523432\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.747934\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.427151\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.490101\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.605870\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.606959\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.831074\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.686636\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.435351\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.573076\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.482439\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.649531\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.514821\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.575691\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.463383\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.681268\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.527040\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.342920\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.676554\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.657062\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.404022\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.489471\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.556943\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.627485\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.401532\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.590753\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.528762\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.572479\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.359150\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.804281\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.522894\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.406502\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.431553\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.448504\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.556332\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.432184\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.660859\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.733443\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.361624\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.462046\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.479371\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.355766\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.323462\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.628630\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.576751\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.400771\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.468115\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.456440\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.420521\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.457067\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.423380\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.875923\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.441314\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.418748\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.254510\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.440969\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.568233\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.394811\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.592721\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.465203\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.358308\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.609424\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.429101\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.523556\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.491583\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.839469\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.439648\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.377343\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.301048\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.610932\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.548298\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.334167\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.610236\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.310153\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.438443\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.581521\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.325235\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.219351\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.509756\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.386193\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.242798\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.454868\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.246763\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.313255\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.517205\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.334310\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.463843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.275333\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.242039\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.634821\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.462529\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.442258\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.344737\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.835924\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.268016\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.501315\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.448561\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.489691\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.440701\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.328554\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.319423\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.517890\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.294943\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.196737\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.507358\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.341834\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.541897\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.371037\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.245015\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.419878\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.391371\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.293451\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.452054\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.321190\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.278685\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.454347\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.333042\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.316395\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.382539\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.257236\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.593985\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.529619\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.302655\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.397741\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.334192\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.241605\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.246334\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.353900\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.447630\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.430728\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.312698\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.367204\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.237943\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.373325\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.736196\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.210177\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.335738\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.576619\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.308573\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.297507\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.384591\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.379744\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.355040\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.374278\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.302295\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.329058\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.204561\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.379615\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.481697\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.218928\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.127365\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.308933\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.249083\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.427453\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.422403\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.209285\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.413395\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.183534\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.430377\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.565923\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.398166\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.290346\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.438657\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.343528\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.224384\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.150797\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.379250\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.505016\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.411254\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.364327\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.242068\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.358919\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.231666\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.467407\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.869374\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.168314\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.387613\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.272731\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.223212\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.435378\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.262027\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.500027\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.225742\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.328058\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.249375\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.241901\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.327965\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.435415\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.481110\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.300700\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.537746\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.173806\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.341718\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.342351\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.381223\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.354264\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.343546\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.166615\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.170888\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.264435\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.454773\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.415338\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.237977\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.247313\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.352135\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.376562\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.287003\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.478117\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.169878\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.308380\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.417408\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.162424\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.146630\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.317456\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.230558\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.274058\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.350653\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.308995\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.382225\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.130602\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.394411\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.569951\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.148920\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.440303\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.217982\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.261084\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.503911\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.312251\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.107222\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.400702\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.262287\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.260740\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.287202\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.269131\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.211359\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.293329\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.148806\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.326923\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.350232\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.234421\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.501726\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.259064\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.160998\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.316133\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.524212\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.203323\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.221418\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.624271\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.261390\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.227903\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.431187\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.203269\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.191490\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.484678\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.418483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.470225\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.305342\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.254001\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.429034\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.184692\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.236413\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.398042\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.294743\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.219517\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.376666\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.301341\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.358882\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.162970\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.345822\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.213883\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.278343\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.355168\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.145276\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.618676\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.211178\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.234764\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.507627\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.171299\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.194010\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.253382\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.208232\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.222904\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.220216\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.266367\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.094058\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.179380\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.246824\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.310075\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.363906\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.325184\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.237055\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.186761\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.192746\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.211503\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.278102\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.173907\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.514868\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.440335\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.207410\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.457287\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.337085\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.335786\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.346658\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.353555\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.238561\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.275323\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.256651\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.269205\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.289131\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.506465\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.124955\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.137856\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.216183\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.239745\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.262751\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.294379\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.316894\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.225781\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.169936\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.211895\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.407390\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.122063\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.427096\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.215506\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.324968\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.415560\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.127296\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.219732\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.519885\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.110518\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.504166\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.249237\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.321947\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.264053\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.159087\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.581821\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.423287\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.337408\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.321764\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.112309\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.127221\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.259293\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.356923\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.149129\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.184730\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.265536\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.184321\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.503277\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.135164\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.156102\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.211608\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.155500\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.442757\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.194349\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.171214\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.248489\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.287070\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.374173\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.129266\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.095459\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.193020\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.446486\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.171752\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.138589\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.255271\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.179747\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.358201\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.169796\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.155462\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.110358\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.188727\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.246195\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.180253\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.326971\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.312121\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.366028\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.260428\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.133660\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.243087\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.244056\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.222017\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.355714\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.217136\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.303426\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.289351\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.381783\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.212762\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.381972\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.115234\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.160905\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.266412\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.173062\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.245626\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.214697\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.259732\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.241296\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.253072\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.346366\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.355444\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.207854\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.251602\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.185261\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.307331\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.348569\n",
      "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.267426\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.120592\n",
      "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.125910\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.246693\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.489647\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.223741\n",
      "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.302583\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.090172\n",
      "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.214767\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.144390\n",
      "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.361525\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.354203\n",
      "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.298606\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.160919\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.232042\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.241350\n",
      "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.216770\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.201445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.391959\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.365946\n",
      "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.168298\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.243284\n",
      "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.118789\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.295866\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.331955\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.297252\n",
      "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.242605\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.292438\n",
      "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.147273\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.270101\n",
      "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.248229\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.354020\n",
      "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.105562\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.188344\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.248080\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.146264\n",
      "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.236892\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.233958\n",
      "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.228540\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.308664\n",
      "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.152722\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.107485\n",
      "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.177862\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.168573\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.534030\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.189022\n",
      "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.158616\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.115295\n",
      "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.223747\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.184981\n",
      "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.202798\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.201643\n",
      "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.276239\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.334889\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.424986\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.310207\n",
      "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.330946\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.491607\n",
      "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.226951\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.170366\n",
      "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.187100\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.153314\n",
      "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.195596\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.314222\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.152412\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.295083\n",
      "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.222639\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.169435\n",
      "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.154599\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.207969\n",
      "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.191234\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.097137\n",
      "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.086411\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.234124\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.433318\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.309792\n",
      "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.226765\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.256878\n",
      "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.161821\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.284457\n",
      "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.249323\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.200527\n",
      "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.232818\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.344866\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.299795\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.203999\n",
      "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.563045\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.136107\n",
      "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.195159\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.161566\n",
      "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.306242\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.321676\n",
      "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.206917\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.220756\n",
      "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.210685\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.177393\n",
      "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.251859\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.253416\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.235485\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.192683\n",
      "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.164362\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.203952\n",
      "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.282136\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.313980\n",
      "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.182300\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.298244\n",
      "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.221611\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.351181\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.130750\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.297332\n",
      "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.213797\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.204616\n",
      "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.160756\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.303672\n",
      "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.137686\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.352657\n",
      "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.280243\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.088621\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.229077\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.128907\n",
      "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.228515\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.201237\n",
      "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.141742\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.187034\n",
      "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.130250\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.097582\n",
      "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.129298\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.402085\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.124546\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.199111\n",
      "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.245475\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.321188\n",
      "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.077399\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.211447\n",
      "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.192371\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.225663\n",
      "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.167921\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.226791\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.203528\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.228631\n",
      "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.105762\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.224763\n",
      "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.194459\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.204288\n",
      "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.262624\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.235158\n",
      "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.275632\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.235371\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.170583\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.091541\n",
      "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.148420\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.326476\n",
      "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.252582\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.204869\n",
      "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.229447\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.176721\n",
      "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.321596\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.136422\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.170886\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.131611\n",
      "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.207542\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.289996\n",
      "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.252232\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.304091\n",
      "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.283298\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.253231\n",
      "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.131658\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.236048\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.420705\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.317665\n",
      "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.220844\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.263768\n",
      "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.197676\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.157939\n",
      "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.166350\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.576240\n",
      "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.169786\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.109166\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.200182\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.216469\n",
      "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.239037\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.536917\n",
      "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.165987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.424974\n",
      "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.364236\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.394507\n",
      "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.271319\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.210873\n",
      "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.255128\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.123189\n",
      "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.347625\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.292026\n",
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.088119\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.505897\n",
      "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.249006\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.201825\n",
      "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.208787\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.365233\n",
      "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.476650\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.169739\n",
      "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.141917\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.147912\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.245088\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.166215\n",
      "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.410836\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.379003\n",
      "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.143937\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.237721\n",
      "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.176180\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.093246\n",
      "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.150727\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.276080\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.264148\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.216543\n",
      "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.136419\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.170968\n",
      "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.257437\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.270401\n",
      "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.198329\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.188250\n",
      "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.111032\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.169048\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.377725\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.295781\n",
      "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.410996\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.176753\n",
      "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.382736\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.286536\n",
      "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.187762\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.362212\n",
      "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.184085\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.193512\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.113305\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.407864\n",
      "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.166755\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.350317\n",
      "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.193955\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.255930\n",
      "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.194739\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.183496\n",
      "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.141593\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.145272\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.146067\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.112534\n",
      "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.214352\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.302698\n",
      "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.108151\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.306184\n",
      "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.141916\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.251551\n",
      "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.251854\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.228091\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.135556\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.232494\n",
      "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.173125\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.057141\n",
      "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.125168\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.128220\n",
      "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.248790\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.208156\n",
      "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.143926\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.210339\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.172039\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.179096\n",
      "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.223848\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.185257\n",
      "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.107696\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.278641\n",
      "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.178020\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.274288\n",
      "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.121979\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.125802\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.411725\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.243760\n",
      "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.239840\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.086942\n",
      "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.271184\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.292195\n",
      "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.097926\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.223933\n",
      "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.176214\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.129501\n",
      "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.204258\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.300450\n",
      "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.071365\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.082822\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.254267\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.158345\n",
      "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.255415\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.189739\n",
      "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.217181\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.160199\n",
      "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.210847\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.192357\n",
      "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.122318\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.233671\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.358468\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.118272\n",
      "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.161749\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.399528\n",
      "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.117455\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.169208\n",
      "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.078486\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.144115\n",
      "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.147280\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.206486\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.229626\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.074585\n",
      "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.232579\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.151124\n",
      "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.395673\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.246231\n",
      "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.366895\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.157939\n",
      "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.152014\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.189819\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.218244\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.098771\n",
      "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.108713\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.146226\n",
      "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.114621\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.179075\n",
      "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.170387\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.126902\n",
      "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.216604\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.224136\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.311368\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.091284\n",
      "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.225008\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.174020\n",
      "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.222439\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.116318\n",
      "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.342006\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.132211\n",
      "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.215091\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.202809\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.110725\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.143973\n",
      "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.345836\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.307718\n",
      "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.191655\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.297806\n",
      "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.134834\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.150092\n",
      "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.222974\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.291745\n",
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.366731\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.147352\n",
      "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.395825\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.287543\n",
      "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.241124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.183583\n",
      "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.111447\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.111164\n",
      "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.147898\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.136142\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.389364\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.362135\n",
      "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.113459\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.168150\n",
      "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.192312\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.211337\n",
      "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.231883\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.180859\n",
      "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.118840\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.128793\n",
      "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.273531\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.175710\n",
      "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.145228\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.165737\n",
      "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.112089\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.188381\n",
      "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.275240\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.186242\n",
      "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.241398\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.322083\n",
      "Train Epoch: 10 [640/60000 (1%)]\tLoss: 0.280497\n",
      "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 0.217018\n",
      "Train Epoch: 10 [1920/60000 (3%)]\tLoss: 0.214075\n",
      "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 0.123675\n",
      "Train Epoch: 10 [3200/60000 (5%)]\tLoss: 0.146599\n",
      "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 0.197618\n",
      "Train Epoch: 10 [4480/60000 (7%)]\tLoss: 0.139922\n",
      "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 0.256958\n",
      "Train Epoch: 10 [5760/60000 (10%)]\tLoss: 0.139374\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.142431\n",
      "Train Epoch: 10 [7040/60000 (12%)]\tLoss: 0.084716\n",
      "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 0.127506\n",
      "Train Epoch: 10 [8320/60000 (14%)]\tLoss: 0.113424\n",
      "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 0.130861\n",
      "Train Epoch: 10 [9600/60000 (16%)]\tLoss: 0.466846\n",
      "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.220993\n",
      "Train Epoch: 10 [10880/60000 (18%)]\tLoss: 0.167128\n",
      "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 0.127824\n",
      "Train Epoch: 10 [12160/60000 (20%)]\tLoss: 0.367977\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.156512\n",
      "Train Epoch: 10 [13440/60000 (22%)]\tLoss: 0.137874\n",
      "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 0.384521\n",
      "Train Epoch: 10 [14720/60000 (25%)]\tLoss: 0.283449\n",
      "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 0.118637\n",
      "Train Epoch: 10 [16000/60000 (27%)]\tLoss: 0.272819\n",
      "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 0.137460\n",
      "Train Epoch: 10 [17280/60000 (29%)]\tLoss: 0.365604\n",
      "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 0.072612\n",
      "Train Epoch: 10 [18560/60000 (31%)]\tLoss: 0.181830\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.108230\n",
      "Train Epoch: 10 [19840/60000 (33%)]\tLoss: 0.277706\n",
      "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.131306\n",
      "Train Epoch: 10 [21120/60000 (35%)]\tLoss: 0.217088\n",
      "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 0.599880\n",
      "Train Epoch: 10 [22400/60000 (37%)]\tLoss: 0.181945\n",
      "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 0.193991\n",
      "Train Epoch: 10 [23680/60000 (39%)]\tLoss: 0.297635\n",
      "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 0.250441\n",
      "Train Epoch: 10 [24960/60000 (42%)]\tLoss: 0.116026\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.174656\n",
      "Train Epoch: 10 [26240/60000 (44%)]\tLoss: 0.085562\n",
      "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 0.102479\n",
      "Train Epoch: 10 [27520/60000 (46%)]\tLoss: 0.201007\n",
      "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 0.155419\n",
      "Train Epoch: 10 [28800/60000 (48%)]\tLoss: 0.290040\n",
      "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 0.202060\n",
      "Train Epoch: 10 [30080/60000 (50%)]\tLoss: 0.323194\n",
      "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.319855\n",
      "Train Epoch: 10 [31360/60000 (52%)]\tLoss: 0.026248\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.224496\n",
      "Train Epoch: 10 [32640/60000 (54%)]\tLoss: 0.305559\n",
      "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 0.227743\n",
      "Train Epoch: 10 [33920/60000 (57%)]\tLoss: 0.129610\n",
      "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 0.099610\n",
      "Train Epoch: 10 [35200/60000 (59%)]\tLoss: 0.199091\n",
      "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 0.225500\n",
      "Train Epoch: 10 [36480/60000 (61%)]\tLoss: 0.089638\n",
      "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 0.208964\n",
      "Train Epoch: 10 [37760/60000 (63%)]\tLoss: 0.241041\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.065248\n",
      "Train Epoch: 10 [39040/60000 (65%)]\tLoss: 0.131965\n",
      "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 0.209560\n",
      "Train Epoch: 10 [40320/60000 (67%)]\tLoss: 0.167311\n",
      "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.227109\n",
      "Train Epoch: 10 [41600/60000 (69%)]\tLoss: 0.108575\n",
      "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 0.126305\n",
      "Train Epoch: 10 [42880/60000 (71%)]\tLoss: 0.186140\n",
      "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 0.182132\n",
      "Train Epoch: 10 [44160/60000 (74%)]\tLoss: 0.196225\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.300664\n",
      "Train Epoch: 10 [45440/60000 (76%)]\tLoss: 0.188259\n",
      "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 0.278311\n",
      "Train Epoch: 10 [46720/60000 (78%)]\tLoss: 0.183535\n",
      "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 0.151166\n",
      "Train Epoch: 10 [48000/60000 (80%)]\tLoss: 0.226527\n",
      "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 0.228189\n",
      "Train Epoch: 10 [49280/60000 (82%)]\tLoss: 0.462002\n",
      "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 0.116058\n",
      "Train Epoch: 10 [50560/60000 (84%)]\tLoss: 0.125781\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.204319\n",
      "Train Epoch: 10 [51840/60000 (86%)]\tLoss: 0.093369\n",
      "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 0.377980\n",
      "Train Epoch: 10 [53120/60000 (88%)]\tLoss: 0.130671\n",
      "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 0.166549\n",
      "Train Epoch: 10 [54400/60000 (91%)]\tLoss: 0.220017\n",
      "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 0.317555\n",
      "Train Epoch: 10 [55680/60000 (93%)]\tLoss: 0.168569\n",
      "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 0.172539\n",
      "Train Epoch: 10 [56960/60000 (95%)]\tLoss: 0.400628\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.409972\n",
      "Train Epoch: 10 [58240/60000 (97%)]\tLoss: 0.131883\n",
      "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 0.187810\n",
      "Train Epoch: 10 [59520/60000 (99%)]\tLoss: 0.162065\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.205744\n",
      "Train Epoch: 11 [640/60000 (1%)]\tLoss: 0.253015\n",
      "Train Epoch: 11 [1280/60000 (2%)]\tLoss: 0.205544\n",
      "Train Epoch: 11 [1920/60000 (3%)]\tLoss: 0.136031\n",
      "Train Epoch: 11 [2560/60000 (4%)]\tLoss: 0.129366\n",
      "Train Epoch: 11 [3200/60000 (5%)]\tLoss: 0.063627\n",
      "Train Epoch: 11 [3840/60000 (6%)]\tLoss: 0.191585\n",
      "Train Epoch: 11 [4480/60000 (7%)]\tLoss: 0.212376\n",
      "Train Epoch: 11 [5120/60000 (9%)]\tLoss: 0.168120\n",
      "Train Epoch: 11 [5760/60000 (10%)]\tLoss: 0.262725\n",
      "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 0.070732\n",
      "Train Epoch: 11 [7040/60000 (12%)]\tLoss: 0.293685\n",
      "Train Epoch: 11 [7680/60000 (13%)]\tLoss: 0.140017\n",
      "Train Epoch: 11 [8320/60000 (14%)]\tLoss: 0.199685\n",
      "Train Epoch: 11 [8960/60000 (15%)]\tLoss: 0.086962\n",
      "Train Epoch: 11 [9600/60000 (16%)]\tLoss: 0.193959\n",
      "Train Epoch: 11 [10240/60000 (17%)]\tLoss: 0.245557\n",
      "Train Epoch: 11 [10880/60000 (18%)]\tLoss: 0.174673\n",
      "Train Epoch: 11 [11520/60000 (19%)]\tLoss: 0.303604\n",
      "Train Epoch: 11 [12160/60000 (20%)]\tLoss: 0.247142\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.212882\n",
      "Train Epoch: 11 [13440/60000 (22%)]\tLoss: 0.276209\n",
      "Train Epoch: 11 [14080/60000 (23%)]\tLoss: 0.179483\n",
      "Train Epoch: 11 [14720/60000 (25%)]\tLoss: 0.194539\n",
      "Train Epoch: 11 [15360/60000 (26%)]\tLoss: 0.136322\n",
      "Train Epoch: 11 [16000/60000 (27%)]\tLoss: 0.065329\n",
      "Train Epoch: 11 [16640/60000 (28%)]\tLoss: 0.127751\n",
      "Train Epoch: 11 [17280/60000 (29%)]\tLoss: 0.216611\n",
      "Train Epoch: 11 [17920/60000 (30%)]\tLoss: 0.174171\n",
      "Train Epoch: 11 [18560/60000 (31%)]\tLoss: 0.264298\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 0.226260\n",
      "Train Epoch: 11 [19840/60000 (33%)]\tLoss: 0.196175\n",
      "Train Epoch: 11 [20480/60000 (34%)]\tLoss: 0.267322\n",
      "Train Epoch: 11 [21120/60000 (35%)]\tLoss: 0.144120\n",
      "Train Epoch: 11 [21760/60000 (36%)]\tLoss: 0.148217\n",
      "Train Epoch: 11 [22400/60000 (37%)]\tLoss: 0.109485\n",
      "Train Epoch: 11 [23040/60000 (38%)]\tLoss: 0.118909\n",
      "Train Epoch: 11 [23680/60000 (39%)]\tLoss: 0.194747\n",
      "Train Epoch: 11 [24320/60000 (41%)]\tLoss: 0.265878\n",
      "Train Epoch: 11 [24960/60000 (42%)]\tLoss: 0.183251\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.310798\n",
      "Train Epoch: 11 [26240/60000 (44%)]\tLoss: 0.148748\n",
      "Train Epoch: 11 [26880/60000 (45%)]\tLoss: 0.100643\n",
      "Train Epoch: 11 [27520/60000 (46%)]\tLoss: 0.178184\n",
      "Train Epoch: 11 [28160/60000 (47%)]\tLoss: 0.366788\n",
      "Train Epoch: 11 [28800/60000 (48%)]\tLoss: 0.142198\n",
      "Train Epoch: 11 [29440/60000 (49%)]\tLoss: 0.560770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 11 [30080/60000 (50%)]\tLoss: 0.121404\n",
      "Train Epoch: 11 [30720/60000 (51%)]\tLoss: 0.210308\n",
      "Train Epoch: 11 [31360/60000 (52%)]\tLoss: 0.082204\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.162541\n",
      "Train Epoch: 11 [32640/60000 (54%)]\tLoss: 0.098723\n",
      "Train Epoch: 11 [33280/60000 (55%)]\tLoss: 0.173377\n",
      "Train Epoch: 11 [33920/60000 (57%)]\tLoss: 0.159947\n",
      "Train Epoch: 11 [34560/60000 (58%)]\tLoss: 0.176865\n",
      "Train Epoch: 11 [35200/60000 (59%)]\tLoss: 0.164018\n",
      "Train Epoch: 11 [35840/60000 (60%)]\tLoss: 0.217420\n",
      "Train Epoch: 11 [36480/60000 (61%)]\tLoss: 0.217367\n",
      "Train Epoch: 11 [37120/60000 (62%)]\tLoss: 0.366144\n",
      "Train Epoch: 11 [37760/60000 (63%)]\tLoss: 0.212525\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.284830\n",
      "Train Epoch: 11 [39040/60000 (65%)]\tLoss: 0.156129\n",
      "Train Epoch: 11 [39680/60000 (66%)]\tLoss: 0.338154\n",
      "Train Epoch: 11 [40320/60000 (67%)]\tLoss: 0.172247\n",
      "Train Epoch: 11 [40960/60000 (68%)]\tLoss: 0.131481\n",
      "Train Epoch: 11 [41600/60000 (69%)]\tLoss: 0.122947\n",
      "Train Epoch: 11 [42240/60000 (70%)]\tLoss: 0.263636\n",
      "Train Epoch: 11 [42880/60000 (71%)]\tLoss: 0.065425\n",
      "Train Epoch: 11 [43520/60000 (72%)]\tLoss: 0.329693\n",
      "Train Epoch: 11 [44160/60000 (74%)]\tLoss: 0.300866\n",
      "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 0.071766\n",
      "Train Epoch: 11 [45440/60000 (76%)]\tLoss: 0.126282\n",
      "Train Epoch: 11 [46080/60000 (77%)]\tLoss: 0.220784\n",
      "Train Epoch: 11 [46720/60000 (78%)]\tLoss: 0.354627\n",
      "Train Epoch: 11 [47360/60000 (79%)]\tLoss: 0.052114\n",
      "Train Epoch: 11 [48000/60000 (80%)]\tLoss: 0.250806\n",
      "Train Epoch: 11 [48640/60000 (81%)]\tLoss: 0.340997\n",
      "Train Epoch: 11 [49280/60000 (82%)]\tLoss: 0.060468\n",
      "Train Epoch: 11 [49920/60000 (83%)]\tLoss: 0.142802\n",
      "Train Epoch: 11 [50560/60000 (84%)]\tLoss: 0.162065\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.175882\n",
      "Train Epoch: 11 [51840/60000 (86%)]\tLoss: 0.369895\n",
      "Train Epoch: 11 [52480/60000 (87%)]\tLoss: 0.140683\n",
      "Train Epoch: 11 [53120/60000 (88%)]\tLoss: 0.283737\n",
      "Train Epoch: 11 [53760/60000 (90%)]\tLoss: 0.093612\n",
      "Train Epoch: 11 [54400/60000 (91%)]\tLoss: 0.089063\n",
      "Train Epoch: 11 [55040/60000 (92%)]\tLoss: 0.150786\n",
      "Train Epoch: 11 [55680/60000 (93%)]\tLoss: 0.273324\n",
      "Train Epoch: 11 [56320/60000 (94%)]\tLoss: 0.165683\n",
      "Train Epoch: 11 [56960/60000 (95%)]\tLoss: 0.171797\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 0.063558\n",
      "Train Epoch: 11 [58240/60000 (97%)]\tLoss: 0.376815\n",
      "Train Epoch: 11 [58880/60000 (98%)]\tLoss: 0.178842\n",
      "Train Epoch: 11 [59520/60000 (99%)]\tLoss: 0.131787\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.105323\n",
      "Train Epoch: 12 [640/60000 (1%)]\tLoss: 0.215288\n",
      "Train Epoch: 12 [1280/60000 (2%)]\tLoss: 0.141680\n",
      "Train Epoch: 12 [1920/60000 (3%)]\tLoss: 0.122817\n",
      "Train Epoch: 12 [2560/60000 (4%)]\tLoss: 0.159126\n",
      "Train Epoch: 12 [3200/60000 (5%)]\tLoss: 0.261233\n",
      "Train Epoch: 12 [3840/60000 (6%)]\tLoss: 0.217867\n",
      "Train Epoch: 12 [4480/60000 (7%)]\tLoss: 0.075405\n",
      "Train Epoch: 12 [5120/60000 (9%)]\tLoss: 0.085305\n",
      "Train Epoch: 12 [5760/60000 (10%)]\tLoss: 0.139580\n",
      "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 0.133370\n",
      "Train Epoch: 12 [7040/60000 (12%)]\tLoss: 0.228507\n",
      "Train Epoch: 12 [7680/60000 (13%)]\tLoss: 0.245617\n",
      "Train Epoch: 12 [8320/60000 (14%)]\tLoss: 0.109099\n",
      "Train Epoch: 12 [8960/60000 (15%)]\tLoss: 0.051324\n",
      "Train Epoch: 12 [9600/60000 (16%)]\tLoss: 0.134797\n",
      "Train Epoch: 12 [10240/60000 (17%)]\tLoss: 0.227739\n",
      "Train Epoch: 12 [10880/60000 (18%)]\tLoss: 0.267869\n",
      "Train Epoch: 12 [11520/60000 (19%)]\tLoss: 0.199565\n",
      "Train Epoch: 12 [12160/60000 (20%)]\tLoss: 0.340894\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.087149\n",
      "Train Epoch: 12 [13440/60000 (22%)]\tLoss: 0.137853\n",
      "Train Epoch: 12 [14080/60000 (23%)]\tLoss: 0.216421\n",
      "Train Epoch: 12 [14720/60000 (25%)]\tLoss: 0.173856\n",
      "Train Epoch: 12 [15360/60000 (26%)]\tLoss: 0.139321\n",
      "Train Epoch: 12 [16000/60000 (27%)]\tLoss: 0.110274\n",
      "Train Epoch: 12 [16640/60000 (28%)]\tLoss: 0.051783\n",
      "Train Epoch: 12 [17280/60000 (29%)]\tLoss: 0.087736\n",
      "Train Epoch: 12 [17920/60000 (30%)]\tLoss: 0.221496\n",
      "Train Epoch: 12 [18560/60000 (31%)]\tLoss: 0.125993\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 0.173017\n",
      "Train Epoch: 12 [19840/60000 (33%)]\tLoss: 0.236319\n",
      "Train Epoch: 12 [20480/60000 (34%)]\tLoss: 0.132344\n",
      "Train Epoch: 12 [21120/60000 (35%)]\tLoss: 0.116934\n",
      "Train Epoch: 12 [21760/60000 (36%)]\tLoss: 0.235687\n",
      "Train Epoch: 12 [22400/60000 (37%)]\tLoss: 0.172366\n",
      "Train Epoch: 12 [23040/60000 (38%)]\tLoss: 0.138835\n",
      "Train Epoch: 12 [23680/60000 (39%)]\tLoss: 0.111825\n",
      "Train Epoch: 12 [24320/60000 (41%)]\tLoss: 0.103154\n",
      "Train Epoch: 12 [24960/60000 (42%)]\tLoss: 0.121575\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.133052\n",
      "Train Epoch: 12 [26240/60000 (44%)]\tLoss: 0.212931\n",
      "Train Epoch: 12 [26880/60000 (45%)]\tLoss: 0.104005\n",
      "Train Epoch: 12 [27520/60000 (46%)]\tLoss: 0.156336\n",
      "Train Epoch: 12 [28160/60000 (47%)]\tLoss: 0.345251\n",
      "Train Epoch: 12 [28800/60000 (48%)]\tLoss: 0.224578\n",
      "Train Epoch: 12 [29440/60000 (49%)]\tLoss: 0.088307\n",
      "Train Epoch: 12 [30080/60000 (50%)]\tLoss: 0.323930\n",
      "Train Epoch: 12 [30720/60000 (51%)]\tLoss: 0.197886\n",
      "Train Epoch: 12 [31360/60000 (52%)]\tLoss: 0.156301\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.104767\n",
      "Train Epoch: 12 [32640/60000 (54%)]\tLoss: 0.215121\n",
      "Train Epoch: 12 [33280/60000 (55%)]\tLoss: 0.205689\n",
      "Train Epoch: 12 [33920/60000 (57%)]\tLoss: 0.155997\n",
      "Train Epoch: 12 [34560/60000 (58%)]\tLoss: 0.308870\n",
      "Train Epoch: 12 [35200/60000 (59%)]\tLoss: 0.199898\n",
      "Train Epoch: 12 [35840/60000 (60%)]\tLoss: 0.263309\n",
      "Train Epoch: 12 [36480/60000 (61%)]\tLoss: 0.178878\n",
      "Train Epoch: 12 [37120/60000 (62%)]\tLoss: 0.127948\n",
      "Train Epoch: 12 [37760/60000 (63%)]\tLoss: 0.198405\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.159302\n",
      "Train Epoch: 12 [39040/60000 (65%)]\tLoss: 0.108486\n",
      "Train Epoch: 12 [39680/60000 (66%)]\tLoss: 0.358677\n",
      "Train Epoch: 12 [40320/60000 (67%)]\tLoss: 0.122311\n",
      "Train Epoch: 12 [40960/60000 (68%)]\tLoss: 0.090219\n",
      "Train Epoch: 12 [41600/60000 (69%)]\tLoss: 0.198161\n",
      "Train Epoch: 12 [42240/60000 (70%)]\tLoss: 0.115470\n",
      "Train Epoch: 12 [42880/60000 (71%)]\tLoss: 0.223725\n",
      "Train Epoch: 12 [43520/60000 (72%)]\tLoss: 0.150372\n",
      "Train Epoch: 12 [44160/60000 (74%)]\tLoss: 0.058855\n",
      "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 0.153999\n",
      "Train Epoch: 12 [45440/60000 (76%)]\tLoss: 0.177538\n",
      "Train Epoch: 12 [46080/60000 (77%)]\tLoss: 0.176957\n",
      "Train Epoch: 12 [46720/60000 (78%)]\tLoss: 0.140008\n",
      "Train Epoch: 12 [47360/60000 (79%)]\tLoss: 0.159179\n",
      "Train Epoch: 12 [48000/60000 (80%)]\tLoss: 0.158061\n",
      "Train Epoch: 12 [48640/60000 (81%)]\tLoss: 0.041836\n",
      "Train Epoch: 12 [49280/60000 (82%)]\tLoss: 0.233279\n",
      "Train Epoch: 12 [49920/60000 (83%)]\tLoss: 0.274274\n",
      "Train Epoch: 12 [50560/60000 (84%)]\tLoss: 0.192617\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.156098\n",
      "Train Epoch: 12 [51840/60000 (86%)]\tLoss: 0.188279\n",
      "Train Epoch: 12 [52480/60000 (87%)]\tLoss: 0.191022\n",
      "Train Epoch: 12 [53120/60000 (88%)]\tLoss: 0.168491\n",
      "Train Epoch: 12 [53760/60000 (90%)]\tLoss: 0.204913\n",
      "Train Epoch: 12 [54400/60000 (91%)]\tLoss: 0.154090\n",
      "Train Epoch: 12 [55040/60000 (92%)]\tLoss: 0.210141\n",
      "Train Epoch: 12 [55680/60000 (93%)]\tLoss: 0.105128\n",
      "Train Epoch: 12 [56320/60000 (94%)]\tLoss: 0.238539\n",
      "Train Epoch: 12 [56960/60000 (95%)]\tLoss: 0.072725\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 0.292676\n",
      "Train Epoch: 12 [58240/60000 (97%)]\tLoss: 0.142970\n",
      "Train Epoch: 12 [58880/60000 (98%)]\tLoss: 0.201511\n",
      "Train Epoch: 12 [59520/60000 (99%)]\tLoss: 0.197761\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.200602\n",
      "Train Epoch: 13 [640/60000 (1%)]\tLoss: 0.089665\n",
      "Train Epoch: 13 [1280/60000 (2%)]\tLoss: 0.165857\n",
      "Train Epoch: 13 [1920/60000 (3%)]\tLoss: 0.031883\n",
      "Train Epoch: 13 [2560/60000 (4%)]\tLoss: 0.258375\n",
      "Train Epoch: 13 [3200/60000 (5%)]\tLoss: 0.132076\n",
      "Train Epoch: 13 [3840/60000 (6%)]\tLoss: 0.316956\n",
      "Train Epoch: 13 [4480/60000 (7%)]\tLoss: 0.245175\n",
      "Train Epoch: 13 [5120/60000 (9%)]\tLoss: 0.146021\n",
      "Train Epoch: 13 [5760/60000 (10%)]\tLoss: 0.260315\n",
      "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 0.305233\n",
      "Train Epoch: 13 [7040/60000 (12%)]\tLoss: 0.274496\n",
      "Train Epoch: 13 [7680/60000 (13%)]\tLoss: 0.243734\n",
      "Train Epoch: 13 [8320/60000 (14%)]\tLoss: 0.188962\n",
      "Train Epoch: 13 [8960/60000 (15%)]\tLoss: 0.161888\n",
      "Train Epoch: 13 [9600/60000 (16%)]\tLoss: 0.053957\n",
      "Train Epoch: 13 [10240/60000 (17%)]\tLoss: 0.202935\n",
      "Train Epoch: 13 [10880/60000 (18%)]\tLoss: 0.289532\n",
      "Train Epoch: 13 [11520/60000 (19%)]\tLoss: 0.193367\n",
      "Train Epoch: 13 [12160/60000 (20%)]\tLoss: 0.220840\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.200669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 13 [13440/60000 (22%)]\tLoss: 0.225372\n",
      "Train Epoch: 13 [14080/60000 (23%)]\tLoss: 0.100664\n",
      "Train Epoch: 13 [14720/60000 (25%)]\tLoss: 0.071949\n",
      "Train Epoch: 13 [15360/60000 (26%)]\tLoss: 0.095762\n",
      "Train Epoch: 13 [16000/60000 (27%)]\tLoss: 0.123182\n",
      "Train Epoch: 13 [16640/60000 (28%)]\tLoss: 0.183581\n",
      "Train Epoch: 13 [17280/60000 (29%)]\tLoss: 0.123465\n",
      "Train Epoch: 13 [17920/60000 (30%)]\tLoss: 0.028006\n",
      "Train Epoch: 13 [18560/60000 (31%)]\tLoss: 0.156723\n",
      "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 0.193530\n",
      "Train Epoch: 13 [19840/60000 (33%)]\tLoss: 0.145113\n",
      "Train Epoch: 13 [20480/60000 (34%)]\tLoss: 0.254228\n",
      "Train Epoch: 13 [21120/60000 (35%)]\tLoss: 0.304318\n",
      "Train Epoch: 13 [21760/60000 (36%)]\tLoss: 0.114468\n",
      "Train Epoch: 13 [22400/60000 (37%)]\tLoss: 0.089167\n",
      "Train Epoch: 13 [23040/60000 (38%)]\tLoss: 0.150855\n",
      "Train Epoch: 13 [23680/60000 (39%)]\tLoss: 0.294044\n",
      "Train Epoch: 13 [24320/60000 (41%)]\tLoss: 0.210936\n",
      "Train Epoch: 13 [24960/60000 (42%)]\tLoss: 0.073267\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.396674\n",
      "Train Epoch: 13 [26240/60000 (44%)]\tLoss: 0.318622\n",
      "Train Epoch: 13 [26880/60000 (45%)]\tLoss: 0.184735\n",
      "Train Epoch: 13 [27520/60000 (46%)]\tLoss: 0.446179\n",
      "Train Epoch: 13 [28160/60000 (47%)]\tLoss: 0.143728\n",
      "Train Epoch: 13 [28800/60000 (48%)]\tLoss: 0.166942\n",
      "Train Epoch: 13 [29440/60000 (49%)]\tLoss: 0.190105\n",
      "Train Epoch: 13 [30080/60000 (50%)]\tLoss: 0.107122\n",
      "Train Epoch: 13 [30720/60000 (51%)]\tLoss: 0.139175\n",
      "Train Epoch: 13 [31360/60000 (52%)]\tLoss: 0.078228\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.115850\n",
      "Train Epoch: 13 [32640/60000 (54%)]\tLoss: 0.072375\n",
      "Train Epoch: 13 [33280/60000 (55%)]\tLoss: 0.105192\n",
      "Train Epoch: 13 [33920/60000 (57%)]\tLoss: 0.146930\n",
      "Train Epoch: 13 [34560/60000 (58%)]\tLoss: 0.134060\n",
      "Train Epoch: 13 [35200/60000 (59%)]\tLoss: 0.161768\n",
      "Train Epoch: 13 [35840/60000 (60%)]\tLoss: 0.325773\n",
      "Train Epoch: 13 [36480/60000 (61%)]\tLoss: 0.234011\n",
      "Train Epoch: 13 [37120/60000 (62%)]\tLoss: 0.138261\n",
      "Train Epoch: 13 [37760/60000 (63%)]\tLoss: 0.227489\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.244989\n",
      "Train Epoch: 13 [39040/60000 (65%)]\tLoss: 0.144761\n",
      "Train Epoch: 13 [39680/60000 (66%)]\tLoss: 0.205091\n",
      "Train Epoch: 13 [40320/60000 (67%)]\tLoss: 0.471831\n",
      "Train Epoch: 13 [40960/60000 (68%)]\tLoss: 0.181521\n",
      "Train Epoch: 13 [41600/60000 (69%)]\tLoss: 0.093925\n",
      "Train Epoch: 13 [42240/60000 (70%)]\tLoss: 0.069670\n",
      "Train Epoch: 13 [42880/60000 (71%)]\tLoss: 0.620926\n",
      "Train Epoch: 13 [43520/60000 (72%)]\tLoss: 0.092093\n",
      "Train Epoch: 13 [44160/60000 (74%)]\tLoss: 0.194463\n",
      "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 0.123783\n",
      "Train Epoch: 13 [45440/60000 (76%)]\tLoss: 0.284356\n",
      "Train Epoch: 13 [46080/60000 (77%)]\tLoss: 0.344818\n",
      "Train Epoch: 13 [46720/60000 (78%)]\tLoss: 0.195899\n",
      "Train Epoch: 13 [47360/60000 (79%)]\tLoss: 0.129308\n",
      "Train Epoch: 13 [48000/60000 (80%)]\tLoss: 0.162161\n",
      "Train Epoch: 13 [48640/60000 (81%)]\tLoss: 0.067872\n",
      "Train Epoch: 13 [49280/60000 (82%)]\tLoss: 0.264466\n",
      "Train Epoch: 13 [49920/60000 (83%)]\tLoss: 0.182481\n",
      "Train Epoch: 13 [50560/60000 (84%)]\tLoss: 0.118793\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.116779\n",
      "Train Epoch: 13 [51840/60000 (86%)]\tLoss: 0.250545\n",
      "Train Epoch: 13 [52480/60000 (87%)]\tLoss: 0.267270\n",
      "Train Epoch: 13 [53120/60000 (88%)]\tLoss: 0.143313\n",
      "Train Epoch: 13 [53760/60000 (90%)]\tLoss: 0.186879\n",
      "Train Epoch: 13 [54400/60000 (91%)]\tLoss: 0.237574\n",
      "Train Epoch: 13 [55040/60000 (92%)]\tLoss: 0.088570\n",
      "Train Epoch: 13 [55680/60000 (93%)]\tLoss: 0.061081\n",
      "Train Epoch: 13 [56320/60000 (94%)]\tLoss: 0.050449\n",
      "Train Epoch: 13 [56960/60000 (95%)]\tLoss: 0.205052\n",
      "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 0.045496\n",
      "Train Epoch: 13 [58240/60000 (97%)]\tLoss: 0.197578\n",
      "Train Epoch: 13 [58880/60000 (98%)]\tLoss: 0.212490\n",
      "Train Epoch: 13 [59520/60000 (99%)]\tLoss: 0.270088\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.353729\n",
      "Train Epoch: 14 [640/60000 (1%)]\tLoss: 0.214079\n",
      "Train Epoch: 14 [1280/60000 (2%)]\tLoss: 0.131481\n",
      "Train Epoch: 14 [1920/60000 (3%)]\tLoss: 0.215221\n",
      "Train Epoch: 14 [2560/60000 (4%)]\tLoss: 0.229775\n",
      "Train Epoch: 14 [3200/60000 (5%)]\tLoss: 0.143909\n",
      "Train Epoch: 14 [3840/60000 (6%)]\tLoss: 0.148434\n",
      "Train Epoch: 14 [4480/60000 (7%)]\tLoss: 0.193503\n",
      "Train Epoch: 14 [5120/60000 (9%)]\tLoss: 0.249446\n",
      "Train Epoch: 14 [5760/60000 (10%)]\tLoss: 0.246102\n",
      "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 0.051347\n",
      "Train Epoch: 14 [7040/60000 (12%)]\tLoss: 0.153332\n",
      "Train Epoch: 14 [7680/60000 (13%)]\tLoss: 0.214762\n",
      "Train Epoch: 14 [8320/60000 (14%)]\tLoss: 0.074039\n",
      "Train Epoch: 14 [8960/60000 (15%)]\tLoss: 0.096701\n",
      "Train Epoch: 14 [9600/60000 (16%)]\tLoss: 0.070886\n",
      "Train Epoch: 14 [10240/60000 (17%)]\tLoss: 0.108911\n",
      "Train Epoch: 14 [10880/60000 (18%)]\tLoss: 0.129143\n",
      "Train Epoch: 14 [11520/60000 (19%)]\tLoss: 0.396334\n",
      "Train Epoch: 14 [12160/60000 (20%)]\tLoss: 0.128307\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.200309\n",
      "Train Epoch: 14 [13440/60000 (22%)]\tLoss: 0.155958\n",
      "Train Epoch: 14 [14080/60000 (23%)]\tLoss: 0.196123\n",
      "Train Epoch: 14 [14720/60000 (25%)]\tLoss: 0.101472\n",
      "Train Epoch: 14 [15360/60000 (26%)]\tLoss: 0.261549\n",
      "Train Epoch: 14 [16000/60000 (27%)]\tLoss: 0.134968\n",
      "Train Epoch: 14 [16640/60000 (28%)]\tLoss: 0.170971\n",
      "Train Epoch: 14 [17280/60000 (29%)]\tLoss: 0.061143\n",
      "Train Epoch: 14 [17920/60000 (30%)]\tLoss: 0.188288\n",
      "Train Epoch: 14 [18560/60000 (31%)]\tLoss: 0.075592\n",
      "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 0.317209\n",
      "Train Epoch: 14 [19840/60000 (33%)]\tLoss: 0.276581\n",
      "Train Epoch: 14 [20480/60000 (34%)]\tLoss: 0.229017\n",
      "Train Epoch: 14 [21120/60000 (35%)]\tLoss: 0.343377\n",
      "Train Epoch: 14 [21760/60000 (36%)]\tLoss: 0.183885\n",
      "Train Epoch: 14 [22400/60000 (37%)]\tLoss: 0.112651\n",
      "Train Epoch: 14 [23040/60000 (38%)]\tLoss: 0.195381\n",
      "Train Epoch: 14 [23680/60000 (39%)]\tLoss: 0.206069\n",
      "Train Epoch: 14 [24320/60000 (41%)]\tLoss: 0.090712\n",
      "Train Epoch: 14 [24960/60000 (42%)]\tLoss: 0.114506\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.169033\n",
      "Train Epoch: 14 [26240/60000 (44%)]\tLoss: 0.291505\n",
      "Train Epoch: 14 [26880/60000 (45%)]\tLoss: 0.093113\n",
      "Train Epoch: 14 [27520/60000 (46%)]\tLoss: 0.277829\n",
      "Train Epoch: 14 [28160/60000 (47%)]\tLoss: 0.144759\n",
      "Train Epoch: 14 [28800/60000 (48%)]\tLoss: 0.165712\n",
      "Train Epoch: 14 [29440/60000 (49%)]\tLoss: 0.379401\n",
      "Train Epoch: 14 [30080/60000 (50%)]\tLoss: 0.104125\n",
      "Train Epoch: 14 [30720/60000 (51%)]\tLoss: 0.219705\n",
      "Train Epoch: 14 [31360/60000 (52%)]\tLoss: 0.078532\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.287462\n",
      "Train Epoch: 14 [32640/60000 (54%)]\tLoss: 0.094946\n",
      "Train Epoch: 14 [33280/60000 (55%)]\tLoss: 0.169011\n",
      "Train Epoch: 14 [33920/60000 (57%)]\tLoss: 0.201868\n",
      "Train Epoch: 14 [34560/60000 (58%)]\tLoss: 0.367962\n",
      "Train Epoch: 14 [35200/60000 (59%)]\tLoss: 0.208290\n",
      "Train Epoch: 14 [35840/60000 (60%)]\tLoss: 0.145315\n",
      "Train Epoch: 14 [36480/60000 (61%)]\tLoss: 0.100348\n",
      "Train Epoch: 14 [37120/60000 (62%)]\tLoss: 0.177372\n",
      "Train Epoch: 14 [37760/60000 (63%)]\tLoss: 0.162397\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.069688\n",
      "Train Epoch: 14 [39040/60000 (65%)]\tLoss: 0.256054\n",
      "Train Epoch: 14 [39680/60000 (66%)]\tLoss: 0.080220\n",
      "Train Epoch: 14 [40320/60000 (67%)]\tLoss: 0.079743\n",
      "Train Epoch: 14 [40960/60000 (68%)]\tLoss: 0.124084\n",
      "Train Epoch: 14 [41600/60000 (69%)]\tLoss: 0.133863\n",
      "Train Epoch: 14 [42240/60000 (70%)]\tLoss: 0.371006\n",
      "Train Epoch: 14 [42880/60000 (71%)]\tLoss: 0.139579\n",
      "Train Epoch: 14 [43520/60000 (72%)]\tLoss: 0.200374\n",
      "Train Epoch: 14 [44160/60000 (74%)]\tLoss: 0.162961\n",
      "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 0.158210\n",
      "Train Epoch: 14 [45440/60000 (76%)]\tLoss: 0.080777\n",
      "Train Epoch: 14 [46080/60000 (77%)]\tLoss: 0.231243\n",
      "Train Epoch: 14 [46720/60000 (78%)]\tLoss: 0.218786\n",
      "Train Epoch: 14 [47360/60000 (79%)]\tLoss: 0.321517\n",
      "Train Epoch: 14 [48000/60000 (80%)]\tLoss: 0.115087\n",
      "Train Epoch: 14 [48640/60000 (81%)]\tLoss: 0.109606\n",
      "Train Epoch: 14 [49280/60000 (82%)]\tLoss: 0.230145\n",
      "Train Epoch: 14 [49920/60000 (83%)]\tLoss: 0.100995\n",
      "Train Epoch: 14 [50560/60000 (84%)]\tLoss: 0.077577\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.069812\n",
      "Train Epoch: 14 [51840/60000 (86%)]\tLoss: 0.126073\n",
      "Train Epoch: 14 [52480/60000 (87%)]\tLoss: 0.288875\n",
      "Train Epoch: 14 [53120/60000 (88%)]\tLoss: 0.134771\n",
      "Train Epoch: 14 [53760/60000 (90%)]\tLoss: 0.042814\n",
      "Train Epoch: 14 [54400/60000 (91%)]\tLoss: 0.157971\n",
      "Train Epoch: 14 [55040/60000 (92%)]\tLoss: 0.137359\n",
      "Train Epoch: 14 [55680/60000 (93%)]\tLoss: 0.144170\n",
      "Train Epoch: 14 [56320/60000 (94%)]\tLoss: 0.146591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 14 [56960/60000 (95%)]\tLoss: 0.256539\n",
      "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 0.398501\n",
      "Train Epoch: 14 [58240/60000 (97%)]\tLoss: 0.097455\n",
      "Train Epoch: 14 [58880/60000 (98%)]\tLoss: 0.193664\n",
      "Train Epoch: 14 [59520/60000 (99%)]\tLoss: 0.141736\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.152422\n",
      "Train Epoch: 15 [640/60000 (1%)]\tLoss: 0.071476\n",
      "Train Epoch: 15 [1280/60000 (2%)]\tLoss: 0.246132\n",
      "Train Epoch: 15 [1920/60000 (3%)]\tLoss: 0.269264\n",
      "Train Epoch: 15 [2560/60000 (4%)]\tLoss: 0.197252\n",
      "Train Epoch: 15 [3200/60000 (5%)]\tLoss: 0.340721\n",
      "Train Epoch: 15 [3840/60000 (6%)]\tLoss: 0.149260\n",
      "Train Epoch: 15 [4480/60000 (7%)]\tLoss: 0.126879\n",
      "Train Epoch: 15 [5120/60000 (9%)]\tLoss: 0.098612\n",
      "Train Epoch: 15 [5760/60000 (10%)]\tLoss: 0.207564\n",
      "Train Epoch: 15 [6400/60000 (11%)]\tLoss: 0.313911\n",
      "Train Epoch: 15 [7040/60000 (12%)]\tLoss: 0.203356\n",
      "Train Epoch: 15 [7680/60000 (13%)]\tLoss: 0.224852\n",
      "Train Epoch: 15 [8320/60000 (14%)]\tLoss: 0.101705\n",
      "Train Epoch: 15 [8960/60000 (15%)]\tLoss: 0.090037\n",
      "Train Epoch: 15 [9600/60000 (16%)]\tLoss: 0.153028\n",
      "Train Epoch: 15 [10240/60000 (17%)]\tLoss: 0.130320\n",
      "Train Epoch: 15 [10880/60000 (18%)]\tLoss: 0.094915\n",
      "Train Epoch: 15 [11520/60000 (19%)]\tLoss: 0.129797\n",
      "Train Epoch: 15 [12160/60000 (20%)]\tLoss: 0.159867\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 0.144273\n",
      "Train Epoch: 15 [13440/60000 (22%)]\tLoss: 0.193478\n",
      "Train Epoch: 15 [14080/60000 (23%)]\tLoss: 0.184875\n",
      "Train Epoch: 15 [14720/60000 (25%)]\tLoss: 0.019650\n",
      "Train Epoch: 15 [15360/60000 (26%)]\tLoss: 0.123215\n",
      "Train Epoch: 15 [16000/60000 (27%)]\tLoss: 0.205776\n",
      "Train Epoch: 15 [16640/60000 (28%)]\tLoss: 0.124695\n",
      "Train Epoch: 15 [17280/60000 (29%)]\tLoss: 0.156150\n",
      "Train Epoch: 15 [17920/60000 (30%)]\tLoss: 0.135988\n",
      "Train Epoch: 15 [18560/60000 (31%)]\tLoss: 0.182481\n",
      "Train Epoch: 15 [19200/60000 (32%)]\tLoss: 0.206185\n",
      "Train Epoch: 15 [19840/60000 (33%)]\tLoss: 0.222497\n",
      "Train Epoch: 15 [20480/60000 (34%)]\tLoss: 0.199155\n",
      "Train Epoch: 15 [21120/60000 (35%)]\tLoss: 0.278802\n",
      "Train Epoch: 15 [21760/60000 (36%)]\tLoss: 0.150268\n",
      "Train Epoch: 15 [22400/60000 (37%)]\tLoss: 0.232791\n",
      "Train Epoch: 15 [23040/60000 (38%)]\tLoss: 0.105329\n",
      "Train Epoch: 15 [23680/60000 (39%)]\tLoss: 0.088852\n",
      "Train Epoch: 15 [24320/60000 (41%)]\tLoss: 0.226262\n",
      "Train Epoch: 15 [24960/60000 (42%)]\tLoss: 0.097447\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.166962\n",
      "Train Epoch: 15 [26240/60000 (44%)]\tLoss: 0.182638\n",
      "Train Epoch: 15 [26880/60000 (45%)]\tLoss: 0.154753\n",
      "Train Epoch: 15 [27520/60000 (46%)]\tLoss: 0.393851\n",
      "Train Epoch: 15 [28160/60000 (47%)]\tLoss: 0.108563\n",
      "Train Epoch: 15 [28800/60000 (48%)]\tLoss: 0.047059\n",
      "Train Epoch: 15 [29440/60000 (49%)]\tLoss: 0.163598\n",
      "Train Epoch: 15 [30080/60000 (50%)]\tLoss: 0.261064\n",
      "Train Epoch: 15 [30720/60000 (51%)]\tLoss: 0.141796\n",
      "Train Epoch: 15 [31360/60000 (52%)]\tLoss: 0.121202\n",
      "Train Epoch: 15 [32000/60000 (53%)]\tLoss: 0.177975\n",
      "Train Epoch: 15 [32640/60000 (54%)]\tLoss: 0.073020\n",
      "Train Epoch: 15 [33280/60000 (55%)]\tLoss: 0.333368\n",
      "Train Epoch: 15 [33920/60000 (57%)]\tLoss: 0.150412\n",
      "Train Epoch: 15 [34560/60000 (58%)]\tLoss: 0.063515\n",
      "Train Epoch: 15 [35200/60000 (59%)]\tLoss: 0.136420\n",
      "Train Epoch: 15 [35840/60000 (60%)]\tLoss: 0.159822\n",
      "Train Epoch: 15 [36480/60000 (61%)]\tLoss: 0.068980\n",
      "Train Epoch: 15 [37120/60000 (62%)]\tLoss: 0.267130\n",
      "Train Epoch: 15 [37760/60000 (63%)]\tLoss: 0.187593\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 0.113933\n",
      "Train Epoch: 15 [39040/60000 (65%)]\tLoss: 0.128079\n",
      "Train Epoch: 15 [39680/60000 (66%)]\tLoss: 0.103644\n",
      "Train Epoch: 15 [40320/60000 (67%)]\tLoss: 0.173265\n",
      "Train Epoch: 15 [40960/60000 (68%)]\tLoss: 0.197473\n",
      "Train Epoch: 15 [41600/60000 (69%)]\tLoss: 0.150455\n",
      "Train Epoch: 15 [42240/60000 (70%)]\tLoss: 0.177604\n",
      "Train Epoch: 15 [42880/60000 (71%)]\tLoss: 0.206253\n",
      "Train Epoch: 15 [43520/60000 (72%)]\tLoss: 0.141570\n",
      "Train Epoch: 15 [44160/60000 (74%)]\tLoss: 0.100795\n",
      "Train Epoch: 15 [44800/60000 (75%)]\tLoss: 0.335735\n",
      "Train Epoch: 15 [45440/60000 (76%)]\tLoss: 0.090584\n",
      "Train Epoch: 15 [46080/60000 (77%)]\tLoss: 0.247779\n",
      "Train Epoch: 15 [46720/60000 (78%)]\tLoss: 0.148348\n",
      "Train Epoch: 15 [47360/60000 (79%)]\tLoss: 0.153480\n",
      "Train Epoch: 15 [48000/60000 (80%)]\tLoss: 0.169981\n",
      "Train Epoch: 15 [48640/60000 (81%)]\tLoss: 0.220864\n",
      "Train Epoch: 15 [49280/60000 (82%)]\tLoss: 0.140773\n",
      "Train Epoch: 15 [49920/60000 (83%)]\tLoss: 0.070248\n",
      "Train Epoch: 15 [50560/60000 (84%)]\tLoss: 0.115156\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.104866\n",
      "Train Epoch: 15 [51840/60000 (86%)]\tLoss: 0.132362\n",
      "Train Epoch: 15 [52480/60000 (87%)]\tLoss: 0.116849\n",
      "Train Epoch: 15 [53120/60000 (88%)]\tLoss: 0.093605\n",
      "Train Epoch: 15 [53760/60000 (90%)]\tLoss: 0.259948\n",
      "Train Epoch: 15 [54400/60000 (91%)]\tLoss: 0.132085\n",
      "Train Epoch: 15 [55040/60000 (92%)]\tLoss: 0.206282\n",
      "Train Epoch: 15 [55680/60000 (93%)]\tLoss: 0.349775\n",
      "Train Epoch: 15 [56320/60000 (94%)]\tLoss: 0.121176\n",
      "Train Epoch: 15 [56960/60000 (95%)]\tLoss: 0.025711\n",
      "Train Epoch: 15 [57600/60000 (96%)]\tLoss: 0.383777\n",
      "Train Epoch: 15 [58240/60000 (97%)]\tLoss: 0.164647\n",
      "Train Epoch: 15 [58880/60000 (98%)]\tLoss: 0.298653\n",
      "Train Epoch: 15 [59520/60000 (99%)]\tLoss: 0.068690\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.077857\n",
      "Train Epoch: 16 [640/60000 (1%)]\tLoss: 0.296681\n",
      "Train Epoch: 16 [1280/60000 (2%)]\tLoss: 0.176484\n",
      "Train Epoch: 16 [1920/60000 (3%)]\tLoss: 0.093753\n",
      "Train Epoch: 16 [2560/60000 (4%)]\tLoss: 0.149925\n",
      "Train Epoch: 16 [3200/60000 (5%)]\tLoss: 0.154075\n",
      "Train Epoch: 16 [3840/60000 (6%)]\tLoss: 0.296235\n",
      "Train Epoch: 16 [4480/60000 (7%)]\tLoss: 0.067960\n",
      "Train Epoch: 16 [5120/60000 (9%)]\tLoss: 0.247180\n",
      "Train Epoch: 16 [5760/60000 (10%)]\tLoss: 0.205700\n",
      "Train Epoch: 16 [6400/60000 (11%)]\tLoss: 0.257754\n",
      "Train Epoch: 16 [7040/60000 (12%)]\tLoss: 0.187202\n",
      "Train Epoch: 16 [7680/60000 (13%)]\tLoss: 0.127557\n",
      "Train Epoch: 16 [8320/60000 (14%)]\tLoss: 0.167758\n",
      "Train Epoch: 16 [8960/60000 (15%)]\tLoss: 0.222455\n",
      "Train Epoch: 16 [9600/60000 (16%)]\tLoss: 0.131218\n",
      "Train Epoch: 16 [10240/60000 (17%)]\tLoss: 0.066998\n",
      "Train Epoch: 16 [10880/60000 (18%)]\tLoss: 0.067977\n",
      "Train Epoch: 16 [11520/60000 (19%)]\tLoss: 0.222227\n",
      "Train Epoch: 16 [12160/60000 (20%)]\tLoss: 0.151007\n",
      "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 0.131518\n",
      "Train Epoch: 16 [13440/60000 (22%)]\tLoss: 0.145401\n",
      "Train Epoch: 16 [14080/60000 (23%)]\tLoss: 0.353049\n",
      "Train Epoch: 16 [14720/60000 (25%)]\tLoss: 0.293439\n",
      "Train Epoch: 16 [15360/60000 (26%)]\tLoss: 0.184886\n",
      "Train Epoch: 16 [16000/60000 (27%)]\tLoss: 0.329954\n",
      "Train Epoch: 16 [16640/60000 (28%)]\tLoss: 0.276656\n",
      "Train Epoch: 16 [17280/60000 (29%)]\tLoss: 0.120171\n",
      "Train Epoch: 16 [17920/60000 (30%)]\tLoss: 0.152328\n",
      "Train Epoch: 16 [18560/60000 (31%)]\tLoss: 0.170030\n",
      "Train Epoch: 16 [19200/60000 (32%)]\tLoss: 0.118602\n",
      "Train Epoch: 16 [19840/60000 (33%)]\tLoss: 0.205205\n",
      "Train Epoch: 16 [20480/60000 (34%)]\tLoss: 0.256381\n",
      "Train Epoch: 16 [21120/60000 (35%)]\tLoss: 0.239339\n",
      "Train Epoch: 16 [21760/60000 (36%)]\tLoss: 0.058897\n",
      "Train Epoch: 16 [22400/60000 (37%)]\tLoss: 0.182222\n",
      "Train Epoch: 16 [23040/60000 (38%)]\tLoss: 0.196054\n",
      "Train Epoch: 16 [23680/60000 (39%)]\tLoss: 0.064961\n",
      "Train Epoch: 16 [24320/60000 (41%)]\tLoss: 0.265070\n",
      "Train Epoch: 16 [24960/60000 (42%)]\tLoss: 0.085364\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.387863\n",
      "Train Epoch: 16 [26240/60000 (44%)]\tLoss: 0.284984\n",
      "Train Epoch: 16 [26880/60000 (45%)]\tLoss: 0.119216\n",
      "Train Epoch: 16 [27520/60000 (46%)]\tLoss: 0.188468\n",
      "Train Epoch: 16 [28160/60000 (47%)]\tLoss: 0.135889\n",
      "Train Epoch: 16 [28800/60000 (48%)]\tLoss: 0.040875\n",
      "Train Epoch: 16 [29440/60000 (49%)]\tLoss: 0.293894\n",
      "Train Epoch: 16 [30080/60000 (50%)]\tLoss: 0.069775\n",
      "Train Epoch: 16 [30720/60000 (51%)]\tLoss: 0.141150\n",
      "Train Epoch: 16 [31360/60000 (52%)]\tLoss: 0.121553\n",
      "Train Epoch: 16 [32000/60000 (53%)]\tLoss: 0.192409\n",
      "Train Epoch: 16 [32640/60000 (54%)]\tLoss: 0.316277\n",
      "Train Epoch: 16 [33280/60000 (55%)]\tLoss: 0.234340\n",
      "Train Epoch: 16 [33920/60000 (57%)]\tLoss: 0.137161\n",
      "Train Epoch: 16 [34560/60000 (58%)]\tLoss: 0.222261\n",
      "Train Epoch: 16 [35200/60000 (59%)]\tLoss: 0.210788\n",
      "Train Epoch: 16 [35840/60000 (60%)]\tLoss: 0.158645\n",
      "Train Epoch: 16 [36480/60000 (61%)]\tLoss: 0.117435\n",
      "Train Epoch: 16 [37120/60000 (62%)]\tLoss: 0.186352\n",
      "Train Epoch: 16 [37760/60000 (63%)]\tLoss: 0.199582\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 0.188884\n",
      "Train Epoch: 16 [39040/60000 (65%)]\tLoss: 0.192520\n",
      "Train Epoch: 16 [39680/60000 (66%)]\tLoss: 0.215079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 16 [40320/60000 (67%)]\tLoss: 0.047270\n",
      "Train Epoch: 16 [40960/60000 (68%)]\tLoss: 0.141441\n",
      "Train Epoch: 16 [41600/60000 (69%)]\tLoss: 0.142720\n",
      "Train Epoch: 16 [42240/60000 (70%)]\tLoss: 0.280320\n",
      "Train Epoch: 16 [42880/60000 (71%)]\tLoss: 0.122293\n",
      "Train Epoch: 16 [43520/60000 (72%)]\tLoss: 0.287751\n",
      "Train Epoch: 16 [44160/60000 (74%)]\tLoss: 0.044363\n",
      "Train Epoch: 16 [44800/60000 (75%)]\tLoss: 0.057067\n",
      "Train Epoch: 16 [45440/60000 (76%)]\tLoss: 0.210129\n",
      "Train Epoch: 16 [46080/60000 (77%)]\tLoss: 0.237609\n",
      "Train Epoch: 16 [46720/60000 (78%)]\tLoss: 0.126459\n",
      "Train Epoch: 16 [47360/60000 (79%)]\tLoss: 0.266456\n",
      "Train Epoch: 16 [48000/60000 (80%)]\tLoss: 0.150010\n",
      "Train Epoch: 16 [48640/60000 (81%)]\tLoss: 0.129779\n",
      "Train Epoch: 16 [49280/60000 (82%)]\tLoss: 0.221217\n",
      "Train Epoch: 16 [49920/60000 (83%)]\tLoss: 0.231053\n",
      "Train Epoch: 16 [50560/60000 (84%)]\tLoss: 0.316029\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.191244\n",
      "Train Epoch: 16 [51840/60000 (86%)]\tLoss: 0.110643\n",
      "Train Epoch: 16 [52480/60000 (87%)]\tLoss: 0.042156\n",
      "Train Epoch: 16 [53120/60000 (88%)]\tLoss: 0.151163\n",
      "Train Epoch: 16 [53760/60000 (90%)]\tLoss: 0.099875\n",
      "Train Epoch: 16 [54400/60000 (91%)]\tLoss: 0.261848\n",
      "Train Epoch: 16 [55040/60000 (92%)]\tLoss: 0.140268\n",
      "Train Epoch: 16 [55680/60000 (93%)]\tLoss: 0.167322\n",
      "Train Epoch: 16 [56320/60000 (94%)]\tLoss: 0.195322\n",
      "Train Epoch: 16 [56960/60000 (95%)]\tLoss: 0.052288\n",
      "Train Epoch: 16 [57600/60000 (96%)]\tLoss: 0.501374\n",
      "Train Epoch: 16 [58240/60000 (97%)]\tLoss: 0.077783\n",
      "Train Epoch: 16 [58880/60000 (98%)]\tLoss: 0.271209\n",
      "Train Epoch: 16 [59520/60000 (99%)]\tLoss: 0.075103\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.245868\n",
      "Train Epoch: 17 [640/60000 (1%)]\tLoss: 0.317695\n",
      "Train Epoch: 17 [1280/60000 (2%)]\tLoss: 0.093096\n",
      "Train Epoch: 17 [1920/60000 (3%)]\tLoss: 0.139886\n",
      "Train Epoch: 17 [2560/60000 (4%)]\tLoss: 0.127478\n",
      "Train Epoch: 17 [3200/60000 (5%)]\tLoss: 0.082693\n",
      "Train Epoch: 17 [3840/60000 (6%)]\tLoss: 0.198572\n",
      "Train Epoch: 17 [4480/60000 (7%)]\tLoss: 0.143249\n",
      "Train Epoch: 17 [5120/60000 (9%)]\tLoss: 0.106905\n",
      "Train Epoch: 17 [5760/60000 (10%)]\tLoss: 0.074780\n",
      "Train Epoch: 17 [6400/60000 (11%)]\tLoss: 0.214955\n",
      "Train Epoch: 17 [7040/60000 (12%)]\tLoss: 0.067795\n",
      "Train Epoch: 17 [7680/60000 (13%)]\tLoss: 0.153630\n",
      "Train Epoch: 17 [8320/60000 (14%)]\tLoss: 0.179123\n",
      "Train Epoch: 17 [8960/60000 (15%)]\tLoss: 0.225958\n",
      "Train Epoch: 17 [9600/60000 (16%)]\tLoss: 0.155340\n",
      "Train Epoch: 17 [10240/60000 (17%)]\tLoss: 0.198940\n",
      "Train Epoch: 17 [10880/60000 (18%)]\tLoss: 0.161954\n",
      "Train Epoch: 17 [11520/60000 (19%)]\tLoss: 0.103083\n",
      "Train Epoch: 17 [12160/60000 (20%)]\tLoss: 0.138337\n",
      "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 0.186589\n",
      "Train Epoch: 17 [13440/60000 (22%)]\tLoss: 0.269056\n",
      "Train Epoch: 17 [14080/60000 (23%)]\tLoss: 0.092685\n",
      "Train Epoch: 17 [14720/60000 (25%)]\tLoss: 0.047647\n",
      "Train Epoch: 17 [15360/60000 (26%)]\tLoss: 0.107722\n",
      "Train Epoch: 17 [16000/60000 (27%)]\tLoss: 0.103241\n",
      "Train Epoch: 17 [16640/60000 (28%)]\tLoss: 0.153439\n",
      "Train Epoch: 17 [17280/60000 (29%)]\tLoss: 0.140087\n",
      "Train Epoch: 17 [17920/60000 (30%)]\tLoss: 0.167077\n",
      "Train Epoch: 17 [18560/60000 (31%)]\tLoss: 0.064506\n",
      "Train Epoch: 17 [19200/60000 (32%)]\tLoss: 0.068467\n",
      "Train Epoch: 17 [19840/60000 (33%)]\tLoss: 0.086233\n",
      "Train Epoch: 17 [20480/60000 (34%)]\tLoss: 0.078452\n",
      "Train Epoch: 17 [21120/60000 (35%)]\tLoss: 0.140536\n",
      "Train Epoch: 17 [21760/60000 (36%)]\tLoss: 0.132511\n",
      "Train Epoch: 17 [22400/60000 (37%)]\tLoss: 0.098785\n",
      "Train Epoch: 17 [23040/60000 (38%)]\tLoss: 0.248323\n",
      "Train Epoch: 17 [23680/60000 (39%)]\tLoss: 0.344679\n",
      "Train Epoch: 17 [24320/60000 (41%)]\tLoss: 0.192289\n",
      "Train Epoch: 17 [24960/60000 (42%)]\tLoss: 0.059508\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.074658\n",
      "Train Epoch: 17 [26240/60000 (44%)]\tLoss: 0.221079\n",
      "Train Epoch: 17 [26880/60000 (45%)]\tLoss: 0.144634\n",
      "Train Epoch: 17 [27520/60000 (46%)]\tLoss: 0.189121\n",
      "Train Epoch: 17 [28160/60000 (47%)]\tLoss: 0.230494\n",
      "Train Epoch: 17 [28800/60000 (48%)]\tLoss: 0.086821\n",
      "Train Epoch: 17 [29440/60000 (49%)]\tLoss: 0.408320\n",
      "Train Epoch: 17 [30080/60000 (50%)]\tLoss: 0.249861\n",
      "Train Epoch: 17 [30720/60000 (51%)]\tLoss: 0.106718\n",
      "Train Epoch: 17 [31360/60000 (52%)]\tLoss: 0.138013\n",
      "Train Epoch: 17 [32000/60000 (53%)]\tLoss: 0.132187\n",
      "Train Epoch: 17 [32640/60000 (54%)]\tLoss: 0.137555\n",
      "Train Epoch: 17 [33280/60000 (55%)]\tLoss: 0.200733\n",
      "Train Epoch: 17 [33920/60000 (57%)]\tLoss: 0.398175\n",
      "Train Epoch: 17 [34560/60000 (58%)]\tLoss: 0.185607\n",
      "Train Epoch: 17 [35200/60000 (59%)]\tLoss: 0.049671\n",
      "Train Epoch: 17 [35840/60000 (60%)]\tLoss: 0.262839\n",
      "Train Epoch: 17 [36480/60000 (61%)]\tLoss: 0.108726\n",
      "Train Epoch: 17 [37120/60000 (62%)]\tLoss: 0.061479\n",
      "Train Epoch: 17 [37760/60000 (63%)]\tLoss: 0.324351\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 0.253009\n",
      "Train Epoch: 17 [39040/60000 (65%)]\tLoss: 0.122134\n",
      "Train Epoch: 17 [39680/60000 (66%)]\tLoss: 0.268316\n",
      "Train Epoch: 17 [40320/60000 (67%)]\tLoss: 0.055608\n",
      "Train Epoch: 17 [40960/60000 (68%)]\tLoss: 0.032846\n",
      "Train Epoch: 17 [41600/60000 (69%)]\tLoss: 0.035927\n",
      "Train Epoch: 17 [42240/60000 (70%)]\tLoss: 0.165357\n",
      "Train Epoch: 17 [42880/60000 (71%)]\tLoss: 0.125657\n",
      "Train Epoch: 17 [43520/60000 (72%)]\tLoss: 0.545150\n",
      "Train Epoch: 17 [44160/60000 (74%)]\tLoss: 0.104147\n",
      "Train Epoch: 17 [44800/60000 (75%)]\tLoss: 0.254719\n",
      "Train Epoch: 17 [45440/60000 (76%)]\tLoss: 0.158220\n",
      "Train Epoch: 17 [46080/60000 (77%)]\tLoss: 0.363298\n",
      "Train Epoch: 17 [46720/60000 (78%)]\tLoss: 0.479924\n",
      "Train Epoch: 17 [47360/60000 (79%)]\tLoss: 0.130073\n",
      "Train Epoch: 17 [48000/60000 (80%)]\tLoss: 0.221648\n",
      "Train Epoch: 17 [48640/60000 (81%)]\tLoss: 0.167757\n",
      "Train Epoch: 17 [49280/60000 (82%)]\tLoss: 0.102204\n",
      "Train Epoch: 17 [49920/60000 (83%)]\tLoss: 0.050081\n",
      "Train Epoch: 17 [50560/60000 (84%)]\tLoss: 0.111467\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.191683\n",
      "Train Epoch: 17 [51840/60000 (86%)]\tLoss: 0.131938\n",
      "Train Epoch: 17 [52480/60000 (87%)]\tLoss: 0.064671\n",
      "Train Epoch: 17 [53120/60000 (88%)]\tLoss: 0.100690\n",
      "Train Epoch: 17 [53760/60000 (90%)]\tLoss: 0.127534\n",
      "Train Epoch: 17 [54400/60000 (91%)]\tLoss: 0.046540\n",
      "Train Epoch: 17 [55040/60000 (92%)]\tLoss: 0.170968\n",
      "Train Epoch: 17 [55680/60000 (93%)]\tLoss: 0.129448\n",
      "Train Epoch: 17 [56320/60000 (94%)]\tLoss: 0.097693\n",
      "Train Epoch: 17 [56960/60000 (95%)]\tLoss: 0.091919\n",
      "Train Epoch: 17 [57600/60000 (96%)]\tLoss: 0.108632\n",
      "Train Epoch: 17 [58240/60000 (97%)]\tLoss: 0.077551\n",
      "Train Epoch: 17 [58880/60000 (98%)]\tLoss: 0.105274\n",
      "Train Epoch: 17 [59520/60000 (99%)]\tLoss: 0.262492\n",
      "\n",
      "Test set: Avg. loss: 0.0066, Accuracy: 1003/10000 (10%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0090, Accuracy: 1998/10000 (20%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0067, Accuracy: 3000/10000 (30%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0079, Accuracy: 3994/10000 (40%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0063, Accuracy: 4996/10000 (50%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0019, Accuracy: 6014/10000 (60%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0036, Accuracy: 7029/10000 (70%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011, Accuracy: 8052/10000 (81%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0017, Accuracy: 9070/10000 (91%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0035, Accuracy: 9841/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test()\n",
    "for epoch in range(1, 18):\n",
    "    train(epoch)\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,\"pyMNIST_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attack = fb.attacks.L2CarliniWagnerAttack() #L2-CW\n",
    "attack = fb.attacks.L2FastGradientAttack()    \n",
    "atkname = \"L2-FGSM\"\n",
    "# attack = fb.attacks.L2DeepFoolAttack()      #L2-DeepFool\n",
    "# attack = fb.attacks.LinfDeepFoolAttack()    #Linf-DeepFool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= model.eval() #changing evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testAcc(model,dataloader):\n",
    "    total = 0\n",
    "    correct =0\n",
    "    model = model.to(device)\n",
    "    for data in dataloader:\n",
    "        images,labels = data\n",
    "        images,labels = images.to(device),labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _,pred = torch.max(outputs,1)\n",
    "        total += labels.size(0)\n",
    "        correct += (pred == labels).sum().item()\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = dict(mean=[0.1307], std=[0.3081], axis=-3)\n",
    "fmodel = fb.PyTorchModel(model,bounds=(0,1))\n",
    "fmodel = fmodel.transform_bounds((0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get variable name \n",
    "def namestr(obj, namespace):\n",
    "    return [name for name in namespace if namespace[name] is obj]\n",
    "\n",
    "def gen_adv_raw(model,dataLoader,fmodel,eps,atkname):\n",
    "    tempStore = torch.empty([0, 1, 28, 28]).to(device)\n",
    "    tempLabel = torch.empty([0]).type(torch.IntTensor).to(device)\n",
    "    dataname = namestr(dataLoader,globals())[0]\n",
    "    ben_acc = 0.0\n",
    "    adv_acc = 0.0\n",
    "    count = 0\n",
    "    for data in dataLoader:\n",
    "        images,labels = data\n",
    "        images,labels = images.to(device),labels.to(device)\n",
    "        for i,x in enumerate(images):\n",
    "            images[i]=(x-x.min()) / (x.max()-x.min())\n",
    "#         raw_advs, clipped_advs, success = attack(fmodel, images, labels, epsilons=eps)\n",
    "        raw_advs, clipped_advs, success = attack(fmodel, images, labels,epsilons=eps)\n",
    "        \n",
    "        index = success.type(torch.BoolTensor)\n",
    "        cp = clipped_advs[index]\n",
    "        labs = labels[index]\n",
    "        labs = labs.type(torch.IntTensor).to(device)\n",
    "        tempStore = torch.cat((tempStore,cp),0)\n",
    "        tempLabel = torch.cat((tempLabel,labs),0)\n",
    "    adv_acc = len(tempStore)/len(dataLoader.dataset)\n",
    "    \n",
    "    filename_data = \"advMnistData-\"+dataname+'-'+str(eps)+\"-\"+atkname+\".pkl\"\n",
    "    filename_label = \"advMnistLabel-\"+dataname+'-'+str(eps)+\"-\"+atkname+\".pkl\"\n",
    "    print(fb.accuracy(fmodel,tempStore,tempLabel))\n",
    "    torch.save(tempStore,filename_data)\n",
    "    torch.save(tempLabel,filename_label)\n",
    "    return ben_acc,adv_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-3a1a5a310de2>:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "Attack method=L2-DeepFool, epsilon=0.01, ben_acc=0.0,adv_acc=0.0172\n",
      "0.0\n",
      "Attack method=L2-DeepFool, epsilon=0.1, ben_acc=0.0,adv_acc=0.023\n",
      "0.0\n",
      "Attack method=L2-DeepFool, epsilon=0.3, ben_acc=0.0,adv_acc=0.0417\n",
      "0.0\n",
      "Attack method=L2-DeepFool, epsilon=0.5, ben_acc=0.0,adv_acc=0.0673\n",
      "0.0\n",
      "Attack method=L2-DeepFool, epsilon=1, ben_acc=0.0,adv_acc=0.2008\n",
      "0.00022084805823396891\n",
      "Attack method=L2-DeepFool, epsilon=1.5, ben_acc=0.0,adv_acc=0.4528\n",
      "0.0002669157984200865\n",
      "Attack method=L2-DeepFool, epsilon=2, ben_acc=0.0,adv_acc=0.7493\n",
      "0.0004326195048633963\n",
      "Attack method=L2-DeepFool, epsilon=2.5, ben_acc=0.0,adv_acc=0.9246\n",
      "0.0004066280380357057\n",
      "Attack method=L2-DeepFool, epsilon=3, ben_acc=0.0,adv_acc=0.9837\n",
      "9.999999747378752e-05\n",
      "Attack method=L2-DeepFool, epsilon=5, ben_acc=0.0,adv_acc=1.0\n"
     ]
    }
   ],
   "source": [
    "# attack = fb.attacks.L2CarliniWagnerAttack() #L2-CW\n",
    "# attack = fb.attacks.L2FastGradientAttack()    \n",
    "\n",
    "attack = fb.attacks.L2DeepFoolAttack()      #L2-DeepFool\n",
    "# attack = fb.attacks.LinfDeepFoolAttack()    #Linf-DeepFool\n",
    "atkname = \"L2-DeepFool\"\n",
    "epsilons=[0.01,0.1,0.3,0.5,1,1.5,2,2.5,3,5]\n",
    "for eps in epsilons:\n",
    "    ben_acc,adv_acc = gen_adv_raw(model,testLoader,fmodel,eps,atkname)\n",
    "    print(f\"Attack method={atkname}, epsilon={eps}, ben_acc={ben_acc},adv_acc={adv_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-3a1a5a310de2>:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "Attack method=Linf-DeepFool, epsilon=0.01, ben_acc=0.0,adv_acc=0.0243\n",
      "0.0\n",
      "Attack method=Linf-DeepFool, epsilon=0.1, ben_acc=0.0,adv_acc=0.2412\n",
      "0.0\n",
      "Attack method=Linf-DeepFool, epsilon=0.15, ben_acc=0.0,adv_acc=0.5729\n",
      "0.0\n",
      "Attack method=Linf-DeepFool, epsilon=0.3, ben_acc=0.0,adv_acc=0.9981\n",
      "0.0\n",
      "Attack method=Linf-DeepFool, epsilon=0.5, ben_acc=0.0,adv_acc=1.0\n",
      "0.0\n",
      "Attack method=Linf-DeepFool, epsilon=1, ben_acc=0.0,adv_acc=1.0\n",
      "0.0\n",
      "Attack method=Linf-DeepFool, epsilon=3, ben_acc=0.0,adv_acc=1.0\n",
      "0.0\n",
      "Attack method=Linf-DeepFool, epsilon=5, ben_acc=0.0,adv_acc=1.0\n"
     ]
    }
   ],
   "source": [
    "# attack = fb.attacks.L2CarliniWagnerAttack() #L2-CW\n",
    "# attack = fb.attacks.L2FastGradientAttack()    \n",
    "\n",
    "# attack = fb.attacks.L2DeepFoolAttack()      #L2-DeepFool\n",
    "attack = fb.attacks.LinfDeepFoolAttack()    #Linf-DeepFool\n",
    "atkname = \"Linf-DeepFool\"\n",
    "epsilons=[0.01,0.1,0.15,0.3,0.5,1,3,5]\n",
    "for eps in epsilons:\n",
    "    ben_acc,adv_acc = gen_adv_raw(model,testLoader,fmodel,eps,atkname)\n",
    "    print(f\"Attack method={atkname}, epsilon={eps}, ben_acc={ben_acc},adv_acc={adv_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-3a1a5a310de2>:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "Attack method=L2FGSM, epsilon=0.01, ben_acc=0.0,adv_acc=0.0171\n",
      "0.0\n",
      "Attack method=L2FGSM, epsilon=0.1, ben_acc=0.0,adv_acc=0.0209\n",
      "0.0\n",
      "Attack method=L2FGSM, epsilon=0.3, ben_acc=0.0,adv_acc=0.0322\n",
      "0.0\n",
      "Attack method=L2FGSM, epsilon=0.5, ben_acc=0.0,adv_acc=0.0491\n",
      "0.0\n",
      "Attack method=L2FGSM, epsilon=1, ben_acc=0.0,adv_acc=0.1094\n",
      "0.0\n",
      "Attack method=L2FGSM, epsilon=2, ben_acc=0.0,adv_acc=0.3256\n",
      "0.0\n",
      "Attack method=L2FGSM, epsilon=3, ben_acc=0.0,adv_acc=0.6027\n",
      "0.0\n",
      "Attack method=L2FGSM, epsilon=5, ben_acc=0.0,adv_acc=0.8782\n"
     ]
    }
   ],
   "source": [
    "# attack = fb.attacks.L2CarliniWagnerAttack() #L2-CW\n",
    "attack = fb.attacks.L2FastGradientAttack()    \n",
    "\n",
    "# attack = fb.attacks.L2DeepFoolAttack()      #L2-DeepFool\n",
    "# attack = fb.attacks.LinfDeepFoolAttack()    #Linf-DeepFool\n",
    "atkname = \"L2FGSM\"\n",
    "epsilons=[0.01,0.1,0.3,0.5,1,2,3,5]\n",
    "for eps in epsilons:\n",
    "    ben_acc,adv_acc = gen_adv_raw(model,testLoader,fmodel,eps,atkname)\n",
    "    print(f\"Attack method={atkname}, epsilon={eps}, ben_acc={ben_acc},adv_acc={adv_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-3a1a5a310de2>:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "Attack method=L2-CW, epsilon=1, ben_acc=0.0,adv_acc=0.2635\n"
     ]
    }
   ],
   "source": [
    "attack = fb.attacks.L2CarliniWagnerAttack(binary_search_steps=9, steps=10000, stepsize=0.01, confidence=0.4, initial_const=0.001, abort_early=True) #L2-CW\n",
    "# attack = fb.attacks.L2FastGradientAttack()    \n",
    "# attack = fb.attacks.L2DeepFoolAttack()      #L2-DeepFool\n",
    "# attack = fb.attacks.LinfDeepFoolAttack()    #Linf-DeepFool\n",
    "atkname = \"L2-CW\"\n",
    "epsilons=[1]\n",
    "for eps in epsilons:\n",
    "    ben_acc,adv_acc = gen_adv_raw(model,testLoader,fmodel,eps,atkname)\n",
    "    print(f\"Attack method={atkname}, epsilon={eps}, ben_acc={ben_acc},adv_acc={adv_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-3a1a5a310de2>:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack method=L2-CW, epsilon=0.01, ben_acc=0.9814094364643097,adv_acc=0.9813117802143096\n",
      "Attack method=L2-CW, epsilon=0.1, ben_acc=0.9814094364643097,adv_acc=0.9768734037876129\n",
      "Attack method=L2-CW, epsilon=0.3, ben_acc=0.9814094364643097,adv_acc=0.9601004421710968\n",
      "Attack method=L2-CW, epsilon=0.5, ben_acc=0.9814094364643097,adv_acc=0.9337890625\n",
      "Attack method=L2-CW, epsilon=1, ben_acc=0.9814094364643097,adv_acc=0.7824796676635742\n",
      "Attack method=L2-CW, epsilon=2, ben_acc=0.9814094364643097,adv_acc=0.10992705672979355\n",
      "Attack method=L2-CW, epsilon=3, ben_acc=0.9814094364643097,adv_acc=0.00146484375\n"
     ]
    }
   ],
   "source": [
    "attack = fb.attacks.L2CarliniWagnerAttack(binary_search_steps=9, steps=10000, stepsize=0.01, confidence=1, initial_const=0.001, abort_early=True) #L2-CW\n",
    "# attack = fb.attacks.L2FastGradientAttack()    \n",
    "# attack = fb.attacks.L2DeepFoolAttack()      #L2-DeepFool\n",
    "# attack = fb.attacks.LinfDeepFoolAttack()    #Linf-DeepFool\n",
    "atkname = \"L2-CW\"\n",
    "epsilons=[0.01,0.1,0.3,0.5,1,2,3]\n",
    "for eps in epsilons:\n",
    "    ben_acc,adv_acc = gen_adv_raw(model,testLoader,fmodel,eps,atkname)\n",
    "    print(f\"Attack method={atkname}, epsilon={eps}, ben_acc={ben_acc},adv_acc={adv_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.5779042 , 0.58188975, 0.605803  , ..., 0.49420774,\n",
       "         0.45036674, 0.41051126],\n",
       "        [0.55399096, 0.5500054 , 0.58188975, ..., 0.49022222,\n",
       "         0.44638118, 0.42246792],\n",
       "        [0.5500054 , 0.5500054 , 0.5779042 , ..., 0.50217885,\n",
       "         0.46630892, 0.42645344],\n",
       "        ...,\n",
       "        [0.21920507, 0.11558086, 0.07173984, ..., 0.09963867,\n",
       "         0.        , 0.10760976],\n",
       "        [0.19130622, 0.14347966, 0.08768203, ..., 0.05181211,\n",
       "         0.06376874, 0.02789883],\n",
       "        [0.16340739, 0.1713785 , 0.12753747, ..., 0.04384102,\n",
       "         0.08369648, 0.03188437]],\n",
       "\n",
       "       [[0.4030604 , 0.39901477, 0.4192429 , ..., 0.3342847 ,\n",
       "         0.31810215, 0.2938284 ],\n",
       "        [0.4030604 , 0.3949691 , 0.41115162, ..., 0.3342847 ,\n",
       "         0.31810215, 0.3059653 ],\n",
       "        [0.3949691 , 0.3909235 , 0.39901477, ..., 0.34642157,\n",
       "         0.3342847 , 0.3100109 ],\n",
       "        ...,\n",
       "        [0.45160797, 0.35451284, 0.3059653 , ..., 0.34237596,\n",
       "         0.20887019, 0.2938284 ],\n",
       "        [0.4192429 , 0.36260408, 0.2938284 , ..., 0.28169152,\n",
       "         0.28169152, 0.20887019],\n",
       "        [0.38283226, 0.374741  , 0.3100109 , ..., 0.26146337,\n",
       "         0.2897828 , 0.22100708]],\n",
       "\n",
       "       [[0.18690896, 0.1788982 , 0.1949197 , ..., 0.1348391 ,\n",
       "         0.1348391 , 0.12282297],\n",
       "        [0.1949197 , 0.15086058, 0.17088746, ..., 0.11481218,\n",
       "         0.1188176 , 0.12682834],\n",
       "        [0.1788982 , 0.12282297, 0.1348391 , ..., 0.12682834,\n",
       "         0.12682834, 0.12282297],\n",
       "        ...,\n",
       "        [0.6995969 , 0.583441  , 0.5393819 , ..., 0.5754303 ,\n",
       "         0.42322603, 0.49932817],\n",
       "        [0.6635485 , 0.583441  , 0.51935506, ..., 0.5113443 ,\n",
       "         0.4953228 , 0.41922066],\n",
       "        [0.63150555, 0.5874464 , 0.51935506, ..., 0.48731205,\n",
       "         0.5073389 , 0.43123677]]], dtype=float32)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "im = images[0].cpu()\n",
    "show(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.4386755 , 0.50605035, 0.46983054, ..., 0.7186191 ,\n",
       "         0.37106797, 0.4673666 ],\n",
       "        [0.42326224, 0.4342678 , 0.38337395, ..., 0.17982589,\n",
       "         0.34792072, 0.5142908 ],\n",
       "        [0.45151532, 0.43659484, 0.4740466 , ..., 0.7933994 ,\n",
       "         0.31743643, 0.46862596],\n",
       "        ...,\n",
       "        [0.4551291 , 0.41671914, 0.4715279 , ..., 0.49120513,\n",
       "         0.45063925, 0.44384292],\n",
       "        [0.4718359 , 0.4697484 , 0.4188169 , ..., 0.45429066,\n",
       "         0.42811823, 0.44531184],\n",
       "        [0.43729982, 0.47628465, 0.42983273, ..., 0.45484677,\n",
       "         0.4462076 , 0.44435793]],\n",
       "\n",
       "       [[0.44728556, 0.5152216 , 0.45360965, ..., 0.71228135,\n",
       "         0.36514086, 0.49043173],\n",
       "        [0.43649903, 0.45063925, 0.4108057 , ..., 0.06983875,\n",
       "         0.34073424, 0.62730306],\n",
       "        [0.47614095, 0.5036275 , 0.5266515 , ..., 0.76406497,\n",
       "         0.35146603, 0.41325596],\n",
       "        ...,\n",
       "        [0.44406876, 0.38535878, 0.49817944, ..., 0.47553864,\n",
       "         0.44586197, 0.44943467],\n",
       "        [0.45507434, 0.48210913, 0.43400773, ..., 0.45314425,\n",
       "         0.46443728, 0.45934513],\n",
       "        [0.41350234, 0.49864483, 0.4324883 , ..., 0.44932514,\n",
       "         0.45470473, 0.44161853]],\n",
       "\n",
       "       [[0.4428847 , 0.46796206, 0.44632053, ..., 0.62069154,\n",
       "         0.36351877, 0.40722275],\n",
       "        [0.42480904, 0.4286692 , 0.41486436, ..., 0.22552837,\n",
       "         0.41614765, 0.51629615],\n",
       "        [0.46728447, 0.48520616, 0.48629096, ..., 0.6276795 ,\n",
       "         0.42383716, 0.45697704],\n",
       "        ...,\n",
       "        [0.4410573 , 0.3975005 , 0.47839954, ..., 0.45720974,\n",
       "         0.4396063 , 0.4385523 ],\n",
       "        [0.4496263 , 0.45923564, 0.44801107, ..., 0.45444465,\n",
       "         0.4646289 , 0.45135105],\n",
       "        [0.4265201 , 0.47886494, 0.44116682, ..., 0.44946206,\n",
       "         0.45449942, 0.44309688]]], dtype=float32)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "im = pertu[0].cpu()\n",
    "show(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_adv1 = copy.deepcopy(advs_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_mask(num):\n",
    "    x_b = np.zeros((32,32))\n",
    "\n",
    "    for i in range(num):\n",
    "        x_axis = np.random.randint(1, 31)\n",
    "        y_axis = np.random.randint(1, 31)\n",
    "        case_radom = np.random.randint(1, 5)\n",
    "        \n",
    "        if (case_radom == 1) :\n",
    "            x_b[x_axis][y_axis] = 1\n",
    "        \n",
    "        elif (case_radom == 2) :\n",
    "            x_b[x_axis][y_axis+1] = 1\n",
    "        \n",
    "        elif (case_radom == 3) :\n",
    "            x_b[x_axis+1][y_axis] = 1\n",
    "        \n",
    "        elif (case_radom == 4) :\n",
    "            x_b[x_axis+1][y_axis+1] = 1\n",
    "    \n",
    "#     x_b_msk = np.img_as_ubyte(x_b)\n",
    "    return x_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_times = 11\n",
    "def get_masks(num):\n",
    "\n",
    "    mask = np.expand_dims(get_single_mask(num), axis = 0)\n",
    "    for k in range(try_times-1):\n",
    "        mask2 = np.expand_dims(get_single_mask(num), axis = 0)\n",
    "        mask = np.concatenate((mask, mask2))\n",
    "    \n",
    "    # print(mask.shape) ->  (11, 224, 224)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mask = get_masks(300)\n",
    "dst_adv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = img_adv1.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = np.transpose(src,[2,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cat'"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes[pred[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.2) /tmp/pip-req-build-13uokl4r/opencv/modules/imgproc/src/color.simd_helpers.hpp:94: error: (-2:Unspecified error) in function 'cv::impl::{anonymous}::CvtHelper<VScn, VDcn, VDepth, sizePolicy>::CvtHelper(cv::InputArray, cv::OutputArray, int) [with VScn = cv::impl::{anonymous}::Set<1>; VDcn = cv::impl::{anonymous}::Set<3, 4>; VDepth = cv::impl::{anonymous}::Set<0, 2, 5>; cv::impl::{anonymous}::SizePolicy sizePolicy = cv::impl::<unnamed>::NONE; cv::InputArray = const cv::_InputArray&; cv::OutputArray = const cv::_OutputArray&]'\n> Unsupported depth of input image:\n>     'VDepth::contains(depth)'\n> where\n>     'depth' is 6 (CV_64F)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-144-7f9f9b6bf7ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrgb_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.5.2) /tmp/pip-req-build-13uokl4r/opencv/modules/imgproc/src/color.simd_helpers.hpp:94: error: (-2:Unspecified error) in function 'cv::impl::{anonymous}::CvtHelper<VScn, VDcn, VDepth, sizePolicy>::CvtHelper(cv::InputArray, cv::OutputArray, int) [with VScn = cv::impl::{anonymous}::Set<1>; VDcn = cv::impl::{anonymous}::Set<3, 4>; VDepth = cv::impl::{anonymous}::Set<0, 2, 5>; cv::impl::{anonymous}::SizePolicy sizePolicy = cv::impl::<unnamed>::NONE; cv::InputArray = const cv::_InputArray&; cv::OutputArray = const cv::_OutputArray&]'\n> Unsupported depth of input image:\n>     'VDepth::contains(depth)'\n> where\n>     'depth' is 6 (CV_64F)\n"
     ]
    }
   ],
   "source": [
    "rgb_img = cv2.cvtColor(np.expand_dims(mask[5],axis=2), cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = np.expand_dims(mask[5],axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3fc22c7550>"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(src,cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3fc22b07f0>"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(dst_adv,cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-3a1a5a310de2>:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9231847524642944"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_adv = torch.load(\"./advsamples/advMnistData-testLoader-3-L2-CW.pkl\")\n",
    "mnist_labs = torch.load(\"./advsamples/advMnistLabel-testLoader-3-L2-CW.pkl\")\n",
    "fmodel = fb.PyTorchModel(model,bounds=(0,1))\n",
    "fb.accuracy(fmodel,mnist_adv,mnist_labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (init_block): ConvBlock(\n",
       "    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activ): ReLU(inplace=True)\n",
       "  )\n",
       "  (stage1): Sequential(\n",
       "    (unit1): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit2): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit3): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit4): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit5): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit6): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit7): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit8): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit9): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit10): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit11): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit12): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit13): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit14): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit15): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit16): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit17): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit18): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (stage2): Sequential(\n",
       "    (unit1): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (identity_conv): ConvBlock(\n",
       "        (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit2): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit3): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit4): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit5): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit6): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit7): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit8): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit9): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit10): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit11): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit12): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit13): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit14): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit15): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit16): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit17): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit18): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (stage3): Sequential(\n",
       "    (unit1): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (identity_conv): ConvBlock(\n",
       "        (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit2): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit3): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit4): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit5): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit6): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit7): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit8): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit9): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit10): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit11): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit12): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit13): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit14): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit15): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit16): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit17): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "    (unit18): ResUnit(\n",
       "      (body): ResBlock(\n",
       "        (conv1): ConvBlock(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activ): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBlock(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (activ): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (final_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)\n",
       ")"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-3a1a5a310de2>:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0004066280380357057"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_adv = torch.load(\"./advMnistData-testLoader-3-L2-DeepFool.pkl\")\n",
    "mnist_labs = torch.load(\"./advMnistLabel-testLoader-3-L2-DeepFool.pkl\")\n",
    "\n",
    "fb.accuracy(fmodel,mnist_adv,mnist_labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
